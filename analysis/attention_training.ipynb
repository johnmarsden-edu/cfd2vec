{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis for cfg2vec applied to code workout dataset\n",
    "\n",
    "This analysis will attempt to train a model that can be\n",
    "used to predict the grade a student will get on a\n",
    "programming assignment given only their code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load in the environment variables from the .env file containing DB info\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "loaded = load_dotenv('../.env')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the different types of embedding methods that could have been used\n",
    "# in embedding the nodes\n",
    "import itertools\n",
    "\n",
    "embedding_methods = [\n",
    "    embedding_method\n",
    "    for embedding_method\n",
    "    in itertools.product(\n",
    "        ['deepwalk', 'node2vec'],\n",
    "        ['FC', 'VC', 'LC', 'NC']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph Embedding Layer\n",
    "\n",
    "### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "### Output\n",
    "\n",
    "The output will be a `batch size x max number of graphs x length of node embedding` tensor representing the graph embedding\n",
    "\n",
    "### Layers\n",
    "\n",
    "#### Weight Generation\n",
    "\n",
    "This will be a Linear or Dense Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `max number of nodes x 1` tensor\n",
    "\n",
    "##### Description\n",
    "\n",
    "The weight generation layer will be used to train up an attention matrix, and will have an inner tensor of size `length of node embedding x 1`\n",
    "\n",
    "#### Softmax Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `max number of nodes x 1` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `max number of nodes x 1` tensor\n",
    "\n",
    "##### Description\n",
    "\n",
    "This will normalize the results from the weight generation layer into a probability distribution.\n",
    "\n",
    "#### Dot Product Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor and a `max number of nodes x 1` tensor\n",
    "\n",
    "##### Operations\n",
    "\n",
    "1. i1 will be transposed across dim 2 and 3\n",
    "2. i1 will be matmul'ed with i2\n",
    "3. i1 will be reshaped to remove the dangling dimension\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `batch size x max number of graphs x length of node embedding` tensor\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(self, node_embedding_length: int):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.get_weights = nn.Linear(node_embedding_length, 1)\n",
    "        # Chose dim = 2 because we want the prob_dist to sum to 1 for each of the graphs\n",
    "        self.make_prob_dist = nn.Softmax(dim = 2)\n",
    "\n",
    "    # batch dims: batch size\n",
    "    #             x max number of graphs\n",
    "    #             x max number of nodes\n",
    "    #             x length of node embedding\n",
    "    def forward(self, batch):\n",
    "        # weights dims: max number of nodes x 1\n",
    "        logging.debug(f'self.get_weights.weight: {self.get_weights.weight}')\n",
    "        weights = self.get_weights(batch)\n",
    "        logging.debug(f'weights.shape: {weights.shape}')\n",
    "        # prob_dist dims: max number of nodes x 1\n",
    "        prob_dist = self.make_prob_dist(weights)\n",
    "        logging.debug(f'prob_dist.shape: {prob_dist.shape}')\n",
    "\n",
    "        # transposed dims: batch size\n",
    "        #                  x max number of graphs\n",
    "        #                  x length of node embedding\n",
    "        #                  x max number of nodes\n",
    "        num_dimensions = len(batch.shape)\n",
    "        transposed = torch.transpose(batch, num_dimensions - 2, num_dimensions - 1)\n",
    "        logging.debug(f'transposed.shape: {transposed.shape}')\n",
    "\n",
    "        # returned dims: batch size\n",
    "        #                x max number of graphs\n",
    "        #                x length of node embedding\n",
    "        graph_embeddings = torch.matmul(transposed, prob_dist).squeeze()\n",
    "        logging.debug(f'graph_embeddings.shape: {graph_embeddings.shape}')\n",
    "        return graph_embeddings\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program Embedding Layer\n",
    "\n",
    "### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "### Output\n",
    "\n",
    "The output will be a `batch size x length of node embedding` tensor\n",
    "\n",
    "### Layers\n",
    "\n",
    "#### Graph Embedding Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `batch size x max number of graphs x length of node embedding` tensor\n",
    "\n",
    "#### Weight Generation Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x length of node embedding` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `max number of graphs x 1` tensor\n",
    "\n",
    "##### Description\n",
    "\n",
    "This layer will generate weights for each programs' graph embeddings so that we can sum them up, and will have an inner matrix of size `length of node embedding x 1`\n",
    "\n",
    "#### Softmax Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `max number of graphs x 1` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `max number of graphs x 1` tensor\n",
    "\n",
    "##### Description\n",
    "\n",
    "This layer will be used to convert the graph weights to a probability distribution\n",
    "\n",
    "#### Input dot product\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x length of node embedding` tensor and a `max number of graphs x 1` tensor\n",
    "\n",
    "##### Operations\n",
    "\n",
    "1. i1 will be transposed across dim 1 and 2\n",
    "2. i1 will be matmul'ed with i2\n",
    "3. i1 will be reshaped to remove the dangling dimension\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `batch size x length of node embedding` tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CfgProgramEmbedding(nn.Module):\n",
    "    def __init__(self, node_embedding_length: int):\n",
    "        super(CfgProgramEmbedding, self).__init__()\n",
    "        self.embed_graphs = GraphEmbedding(node_embedding_length)\n",
    "        self.get_weights = nn.Linear(node_embedding_length, 1)\n",
    "        # Chose dim = 1 because we want to have the prob_dist sum to 1 for each of the graphs\n",
    "        self.make_prob_dist = nn.Softmax(dim = 1)\n",
    "\n",
    "    # batch dims: batch size\n",
    "    #             x max number of graphs\n",
    "    #             x max number of nodes\n",
    "    #             x length of node embedding\n",
    "    def forward(self, batch):\n",
    "        # graph_embeddings dims: batch size\n",
    "        #                        x max number of graphs\n",
    "        #                        x length of node embedding\n",
    "        graph_embeddings = self.embed_graphs(batch)\n",
    "        logging.debug(f'graph_embeddings.shape: {graph_embeddings.shape}')\n",
    "\n",
    "        # weights dims: max number of graphs x 1\n",
    "        weights = self.get_weights(graph_embeddings)\n",
    "        logging.debug(f'weights.shape: {weights.shape}')\n",
    "        # prob_dist dims: max number of graphs x 1\n",
    "        prob_dist = self.make_prob_dist(weights)\n",
    "        logging.debug(f'prob_dist.shape: {prob_dist.shape}')\n",
    "\n",
    "        # transposed dims: batch size\n",
    "        #                  x length of node embedding\n",
    "        #                  x max number of graphs\n",
    "        num_dimensions = len(graph_embeddings.shape)\n",
    "        transposed = torch.transpose(graph_embeddings, num_dimensions - 2, num_dimensions - 1)\n",
    "        logging.debug(f'transposed.shape: {transposed.shape}')\n",
    "\n",
    "        # program_embeddings dims:  batch size\n",
    "        #                           x length of node embedding\n",
    "        program_embeddings = torch.matmul(transposed, prob_dist).squeeze()\n",
    "        logging.debug(f'program_embeddings.shape: {program_embeddings.shape}')\n",
    "        return program_embeddings\n",
    "\n",
    "class Code2VecProgramEmbedding(nn.Module):\n",
    "    def __init__(self, method_embedding_length):\n",
    "        super(Code2VecProgramEmbedding, self).__init__()\n",
    "        self.get_weights = nn.Linear(method_embedding_length, 1)\n",
    "        # Chose dim = 1 because we want to have the prob_dist sum to 1 for each of the graphs\n",
    "        self.make_prob_dist = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # weights dims: max number of graphs x 1\n",
    "        weights = self.get_weights(batch)\n",
    "        logging.debug(f'weights.shape: {weights.shape}')\n",
    "        # prob_dist dims: max number of graphs x 1\n",
    "        prob_dist = self.make_prob_dist(weights)\n",
    "        logging.debug(f'prob_dist.shape: {prob_dist.shape}')\n",
    "\n",
    "        # transposed dims: batch size\n",
    "        #                  x length of graph embedding\n",
    "        #                  x max number of graphs\n",
    "        num_dimensions = len(batch.shape)\n",
    "        transposed = torch.transpose(batch, num_dimensions - 2, num_dimensions - 1)\n",
    "        logging.debug(f'transposed.shape: {transposed.shape}')\n",
    "\n",
    "        # program_embeddings dims:  batch size\n",
    "        #                           x length of graph embedding\n",
    "        program_embeddings = torch.matmul(transposed, prob_dist).squeeze()\n",
    "        logging.debug(f'program_embeddings.shape: {program_embeddings.shape}')\n",
    "        return program_embeddings\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Baseline Naive Model\n",
    "\n",
    "### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "### Output\n",
    "\n",
    "The output will be a `program batch size x 1` tensor with a predicted score for each program\n",
    "\n",
    "### Layers\n",
    "\n",
    "#### Program Embedding Layer\n",
    "\n",
    "##### Input\n",
    "\n",
    "The input will be a `batch size x max number of graphs x max number of nodes x length of node embedding` tensor\n",
    "\n",
    "##### Output\n",
    "\n",
    "The output will be a `batch size x length of node embedding` tensor\n",
    "\n",
    "##### Description\n",
    "\n",
    "This will generate program embeddings for each student program\n",
    "\n",
    "#### Prediction Layer\n",
    "\n",
    "##### Inputs\n",
    "\n",
    "The input will be a `batch size x length of node embedding` tensor\n",
    "\n",
    "##### Outputs\n",
    "\n",
    "The output will be a `batch size x 1` tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class CodeWorkoutPredictor(nn.Module):\n",
    "    def __init__(self, node_embedding_length: int):\n",
    "        super(CodeWorkoutPredictor, self).__init__()\n",
    "        self.embed_programs = CfgProgramEmbedding(node_embedding_length)\n",
    "        self.predict_grades = nn.Linear(node_embedding_length, 1)\n",
    "\n",
    "    # batch dims: batch size\n",
    "    #             x max number of graphs\n",
    "    #             x max number of nodes\n",
    "    #             x length of node embedding\n",
    "    def forward(self, batch):\n",
    "        # program_embeddings dims:  batch size\n",
    "        #                           x length of node embedding\n",
    "        program_embeddings = self.embed_programs(batch)\n",
    "        logging.debug(f'program_embeddings.shape: {program_embeddings.shape}')\n",
    "\n",
    "        # returned dims: batch size\n",
    "        predictions = self.predict_grades(program_embeddings).squeeze()\n",
    "        logging.debug(f'predictions.shape: {predictions.shape}')\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CfgMisconceptionClassifier(nn.Module):\n",
    "    def __init__(self, node_embedding_length: int):\n",
    "        super(CfgMisconceptionClassifier, self).__init__()\n",
    "        self.embed_programs = CfgProgramEmbedding(node_embedding_length)\n",
    "        self.classify_misconceptions = nn.Sequential(nn.Linear(node_embedding_length, 20), nn.ReLU(), nn.Linear(20, 11), nn.Sigmoid())\n",
    "\n",
    "    # batch dims: batch size\n",
    "    #             x max number of graphs\n",
    "    #             x max number of nodes\n",
    "    #             x length of node embedding\n",
    "    def forward(self, batch):\n",
    "        # program_embeddings dims:  batch size\n",
    "        #                           x length of node embedding\n",
    "        program_embeddings = self.embed_programs(batch)\n",
    "        logging.debug(f'program_embeddings.shape: {program_embeddings.shape}')\n",
    "\n",
    "        # returned dims: batch size\n",
    "        classifications = self.classify_misconceptions(program_embeddings).squeeze()\n",
    "        logging.debug(f'predictions.shape: {classifications.shape}')\n",
    "        return classifications\n",
    "\n",
    "class Code2VecMisconceptionClassifier(nn.Module):\n",
    "    def __init__(self, method_embedding_length: int):\n",
    "        super(Code2VecMisconceptionClassifier, self).__init__()\n",
    "        self.embed_programs = CfgProgramEmbedding(method_embedding_length)\n",
    "        self.classify_misconceptions = nn.Sequential(nn.Linear(method_embedding_length, 20), nn.ReLU(), nn.Linear(20, 11), nn.Sigmoid())\n",
    "\n",
    "    # batch dims: batch size\n",
    "    #             x max number of graphs\n",
    "    #             x max number of nodes\n",
    "    #             x length of node embedding\n",
    "    def forward(self, batch):\n",
    "        # program_embeddings dims:  batch size\n",
    "        #                           x length of node embedding\n",
    "        program_embeddings = self.embed_programs(batch)\n",
    "        logging.debug(f'program_embeddings.shape: {program_embeddings.shape}')\n",
    "\n",
    "        # returned dims: batch size\n",
    "        classifications = self.classify_misconceptions(program_embeddings).squeeze()\n",
    "        logging.debug(f'predictions.shape: {classifications.shape}')\n",
    "        return classifications"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from psycopg import Connection, connect as db_connect\n",
    "from dataclasses import dataclass\n",
    "from psycopg.rows import namedtuple_row\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from typing import Optional\n",
    "import torch.utils.data\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logging.info(f'Using {device} device')\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "DROP_IF_EXISTS_MAT_VIEW = 'drop materialized view if exists all_program_nodes'\n",
    "CREATE_MAT_VIEW = '''\n",
    "create materialized view all_program_nodes as\n",
    "select p.strat       as strat,\n",
    "       p.program_id  as program,\n",
    "       p.repo        as repo,\n",
    "       p.commit_id   as commit_id,\n",
    "       g.graph_id    as graph,\n",
    "       n.contents    as content,\n",
    "       n.label       as label,\n",
    "       n.node_type   as ntype,\n",
    "       p.program_num as program_num,\n",
    "       p.train       as train\n",
    "from (select *,\n",
    "             arr[1]                                                      as repo,\n",
    "             arr[array_length(arr, 1)]                                   as commit_id,\n",
    "             rank() over (partition by strat, test order by shuffle_val) as program_num,\n",
    "             not test                                                    as train\n",
    "      from (select *,\n",
    "                   string_to_array(regexp_replace(program_id, '^[VFNL]C-', ''), '_') as arr,\n",
    "                   substring(program_id for 2)                                       as strat,\n",
    "                   random()                                                          as shuffle_val,\n",
    "                   case when random() > .8 then true else false end                  as test\n",
    "            from programs\n",
    "            where program_id like '%Lab12%DecimalToBinary.java%') as sp) as p\n",
    "         join graphs as g on p.id = g.program_id\n",
    "         join nodes as n on g.id = n.graph_id\n",
    "'''\n",
    "\n",
    "CREATE_STRAT_INDEX = 'create index can_strat on all_program_nodes (strat, train)'\n",
    "CREATE_ROW_INDEX = 'create index node_row_num on all_program_nodes (strat, train, program_num)'\n",
    "CREATE_PROGRAM_INDEX = 'create index strat_program on all_program_nodes (strat, train, program)'\n",
    "CREATE_GRAPH_INDEX = 'create index strat_program_graphs on all_program_nodes (strat, train, program, graph)'\n",
    "\n",
    "def create_materialized_view(conn: Connection):\n",
    "    conn.execute(DROP_IF_EXISTS_MAT_VIEW)\n",
    "    conn.execute(CREATE_MAT_VIEW)\n",
    "    conn.execute(CREATE_STRAT_INDEX)\n",
    "    conn.execute(CREATE_ROW_INDEX)\n",
    "    conn.execute(CREATE_PROGRAM_INDEX)\n",
    "    conn.execute(CREATE_GRAPH_INDEX)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "GET_STRAT_DIMENSIONS = '''\n",
    "-- dimension 0 - programs\n",
    "with dim0 as (select count(program) as dim,\n",
    "                     strat,\n",
    "                     train\n",
    "              from (select strat, train, program from all_program_nodes group by strat, train, program) as p\n",
    "              group by strat, train),\n",
    "\n",
    "-- dimension 1 - max number of graphs\n",
    "     dim1 as (select max(num_graphs) as dim,\n",
    "                     strat,\n",
    "                     train\n",
    "              from (select count(*) num_graphs,\n",
    "                           strat,\n",
    "                           train,\n",
    "                           program\n",
    "                    from (select strat,\n",
    "                                 train,\n",
    "                                 program,\n",
    "                                 graph\n",
    "                          from all_program_nodes\n",
    "                          group by strat, train, program, graph) graphs\n",
    "                    group by strat,\n",
    "                             train,\n",
    "                             program) graph_counts\n",
    "              group by strat,\n",
    "                       train),\n",
    "\n",
    "-- dimension 2 - max number of nodes (dimension 3 is the node embeddings which are size 50)\n",
    "     dim2 as (select max(num_graphs) as dim,\n",
    "                     strat,\n",
    "                     train\n",
    "              from (select count(graph) as num_graphs,\n",
    "                           strat,\n",
    "                           train\n",
    "                    from all_program_nodes\n",
    "                    group by strat,\n",
    "                             train,\n",
    "                             program,\n",
    "                             graph\n",
    "                    order by num_graphs desc) num_nodes\n",
    "              group by strat,\n",
    "                       train),\n",
    "-- all strat train pairs and their respective shapes\n",
    "     strat_train_pairs as (select dim0.strat as strat,\n",
    "                                  dim0.train,\n",
    "                                  dim0.dim   as dim0,\n",
    "                                  dim1.dim   as dim1,\n",
    "                                  dim2.dim   as dim2\n",
    "                           from dim0\n",
    "                                    join dim1 on dim0.strat = dim1.strat and dim0.train = dim1.train\n",
    "                                    join dim2 on dim0.strat = dim2.strat and dim0.train = dim2.train)\n",
    "\n",
    "select strat,\n",
    "       train,\n",
    "       dim0,\n",
    "       max(dim1) over (partition by strat) as dim1,\n",
    "       max(dim2) over (partition by strat) as dim2,\n",
    "       50                                  as dim3\n",
    "from strat_train_pairs\n",
    "'''\n",
    "\n",
    "def set_shapes(conn: Connection, shapes: dict[(str, bool), tuple[int, int, int, int]]):\n",
    "    cursor = conn.execute(GET_STRAT_DIMENSIONS)\n",
    "    cursor.row_factory = namedtuple_row\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "    for shape in results:\n",
    "        shapes[(shape.strat, shape.train)] = (shape.dim0, shape.dim1, shape.dim2, shape.dim3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "GET_BATCH = '''\n",
    "select\n",
    "    strat,\n",
    "    program,\n",
    "    tag_id,\n",
    "    graph,\n",
    "    content,\n",
    "    label,\n",
    "    ntype,\n",
    "    program_num\n",
    "from\n",
    "    all_program_nodes\n",
    "where\n",
    "    all_program_nodes.program_num >= %(lower_bound)s and\n",
    "    all_program_nodes.program_num < %(upper_bound)s and\n",
    "    strat = %(canonicalization_strategy)s and\n",
    "    train = %(train)s\n",
    "order by\n",
    "    program_num, strat, program;\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "import csv\n",
    "\n",
    "data_dir = Path.cwd() / '..' / 'data' / 'Data'\n",
    "\n",
    "df = pd.read_csv(data_dir / 'tagged_submissions_lab_12.csv')\n",
    "df.drop(columns=[col for col in df.columns if col.endswith('Output')], inplace=True)\n",
    "df.set_index(keys='id', inplace=True)\n",
    "tags_titles = [c for c in df.columns]\n",
    "df['target'] = [[e for e in x] for _, x in df.iterrows()]\n",
    "df.drop(columns=df.columns[:-1], inplace=True)\n",
    "tags = df.to_dict('index')\n",
    "\n",
    "def get_tags(programs: Iterable[str], ) -> list[int]:\n",
    "    logging.debug(tags)\n",
    "    logging.debug(programs)\n",
    "    return [tags['csc' + tag_id]['target'] for tag_id in programs]\n",
    "\n",
    "\n",
    "def content_string(node_type: str, label: Optional[str], contents: Optional[str]) -> str:\n",
    "    if node_type == 'Source':\n",
    "        return f'Source {contents}'\n",
    "    if node_type == 'Sink':\n",
    "        return f'Sink {contents}'\n",
    "    out_label = f'{label}: ' if label else ''\n",
    "    out_contents = contents if contents else ''\n",
    "    return f'{out_label}{out_contents}'\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, content: str):\n",
    "        self.content: str = content\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes: list[Node] = []\n",
    "\n",
    "\n",
    "class Program:\n",
    "    def __init__(self, program_id):\n",
    "        self.graphs: dict[str, Graph] = {}\n",
    "        self.program_id: str = program_id\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, strat):\n",
    "        self.programs: dict[str, Program] = {}\n",
    "        self.strat = strat + '-'\n",
    "\n",
    "    def to_tensor(self, w2v: Word2Vec, batch_size: int, max_graphs: int, max_nodes: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = np.zeros((batch_size, max_graphs, max_nodes, 50), dtype=np.float32)\n",
    "        program_ids = []\n",
    "        for p_index, (program_id, program) in enumerate(self.programs.items()):\n",
    "            for g_index, graph in enumerate(program.graphs.values()):\n",
    "                for n_index, node in enumerate(graph.nodes):\n",
    "                    if node.content in w2v.wv:\n",
    "                        data[p_index, g_index, n_index] = w2v.wv[node.content]\n",
    "            program_ids.append(program_id[len(self.strat):])\n",
    "\n",
    "        batch_tags = get_tags(program_ids)\n",
    "        logging.debug(batch_tags)\n",
    "        return torch.as_tensor(data, dtype = torch.float32, device = torch.device(device)).squeeze(), torch.tensor(batch_tags, dtype = torch.float32, device = torch.device(device))\n",
    "\n",
    "def get_prog_batch(conn: Connection, lower_bound: int, upper_bound: int, canonicalization_strategy: str, train: bool) -> Batch:\n",
    "    batch = Batch(canonicalization_strategy)\n",
    "    cursor = conn.execute(GET_BATCH, {'lower_bound': lower_bound, 'upper_bound': upper_bound, 'canonicalization_strategy': canonicalization_strategy, 'train': train})\n",
    "    cursor.row_factory = namedtuple_row\n",
    "    for row in cursor:\n",
    "        if not row.tag_id in batch.programs:\n",
    "            batch.programs[row.tag_id] = Program(row.tag_id)\n",
    "        if not row.graph in batch.programs[row.tag_id].graphs:\n",
    "            batch.programs[row.tag_id].graphs[row.graph] = Graph()\n",
    "        batch.programs[row.tag_id].graphs[row.graph].nodes.append(Node(content_string(row.ntype, row.label, row.content)))\n",
    "\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BatchedProgramDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            canonicalization_strategy: str,\n",
    "            w2v_model: str,\n",
    "            batch_size: int,\n",
    "            num_programs: int,\n",
    "            max_graphs: int,\n",
    "            max_nodes: int,\n",
    "            train: bool\n",
    "    ):\n",
    "        super(BatchedProgramDataset, self).__init__()\n",
    "        self.canonicalization_strategy = canonicalization_strategy\n",
    "        self.num_programs = num_programs\n",
    "        self.batch_size = batch_size\n",
    "        self.w2v_model = w2v_model\n",
    "        self.max_graphs = max_graphs\n",
    "        self.max_nodes = max_nodes\n",
    "        self.train = train\n",
    "\n",
    "    def _get_worker_bounds(\n",
    "            self,\n",
    "            worker_id: int,\n",
    "            num_workers: int\n",
    "    ) -> tuple[int, int]:\n",
    "        total_batches = self.num_programs // self.batch_size\n",
    "        (num_batches, batchs_leftover) = divmod(total_batches, num_workers)\n",
    "\n",
    "        def worker_start_bound(num):\n",
    "            offset = num if num < batchs_leftover else batchs_leftover\n",
    "            return self.batch_size * (num_batches * num + offset) + 1\n",
    "\n",
    "        return worker_start_bound(worker_id), worker_start_bound(worker_id + 1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_programs // self.batch_size\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        (lower_bound, upper_bound) = (1, self.num_programs + 1) if worker_info is None else self._get_worker_bounds(worker_info.id, worker_info.num_workers)\n",
    "        conn = db_connect()\n",
    "        w2v = Word2Vec.load(self.w2v_model)\n",
    "        for lower_bound, upper_bound in itertools.pairwise(range(lower_bound, upper_bound + 1, self.batch_size)):\n",
    "            yield get_prog_batch(\n",
    "                conn,\n",
    "                lower_bound,\n",
    "                upper_bound,\n",
    "                self.canonicalization_strategy,\n",
    "                self.train\n",
    "            ).to_tensor(\n",
    "                w2v,\n",
    "                self.batch_size,\n",
    "                self.max_graphs,\n",
    "                self.max_nodes\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "import math\n",
    "\n",
    "\n",
    "class SplitKind(Enum):\n",
    "    Train = auto()\n",
    "    Test = auto()\n",
    "    Validation = auto()\n",
    "\n",
    "\n",
    "train = (None, 0)\n",
    "validation = (None, 0)\n",
    "test = (None, 0)\n",
    "\n",
    "def get_data_split(vector_file: Path, batch_size: int, split: SplitKind) -> tuple[dict[str, pd.Series], int]:\n",
    "    programs = {}\n",
    "    vectors = pd.read_csv(vector_file)\n",
    "    max_methods = 0\n",
    "    for vector in vectors.iterrows():\n",
    "        program_id = vector[1]['CodeStateID']\n",
    "        if program_id not in programs:\n",
    "            programs[program_id] = []\n",
    "        programs[program_id].append(vector[1].drop('CodeStateID'))\n",
    "        max_methods = max(max_methods, len(programs[program_id]))\n",
    "    num_batches = len(programs) // batch_size\n",
    "    test_batches_len = math.floor(num_batches * .2)\n",
    "    val_batches_len = math.floor(num_batches * .1)\n",
    "    train_batches_len = num_batches - test_batches_len - val_batches_len\n",
    "\n",
    "\n",
    "class BatchedCode2VecDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, vector_file: Path, batch_size: int, split: SplitKind):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = get_data_split(vector_file, batch_size, split)\n",
    "        self.max_methods = max_methods\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info:\n",
    "            raise 'You can not use more than one worker on this dataset'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "from typing import Generator\n",
    "\n",
    "save_dir = Path.cwd() / 'save'\n",
    "w2v_model_dir = Path.cwd() / '..' / 'data' / 'models'\n",
    "\n",
    "logging.getLogger().setLevel('CRITICAL')\n",
    "# Add node training dataset\n",
    "\n",
    "strats = ['VC', 'LC', 'FC', 'NC']\n",
    "db_shapes = {}\n",
    "with db_connect() as conn:\n",
    "    set_shapes(conn, db_shapes)\n",
    "\n",
    "DatasetType = tuple[BatchedProgramDataset, BatchedProgramDataset, tuple[int, int, int, int], tuple[int, int, int, int], str, str]\n",
    "\n",
    "def get_graph_datasets(batch_size: int, shapes: dict[(str, bool), tuple[int, int, int, int]]) -> Generator[DatasetType, None, None]:\n",
    "    for embed_method, strat in embedding_methods:\n",
    "        train_shape = shapes[(strat, True)]\n",
    "        test_shape = shapes[(strat, False)]\n",
    "        train_dataset = BatchedProgramDataset(strat, str(w2v_model_dir / f'{strat}-{embed_method}.model'), batch_size, train_shape[0], train_shape[1], train_shape[2], True)\n",
    "        test_dataset = BatchedProgramDataset(strat, str(w2v_model_dir / f'{strat}-{embed_method}.model'), batch_size, test_shape[0], test_shape[1], test_shape[2], False)\n",
    "        yield train_dataset, test_dataset, train_shape, test_shape, strat, embed_method\n",
    "\n",
    "DS_BATCH_SIZE = 20\n",
    "datasets = [(train, test, train_shape, test_shape, strat, embed_method) for train, test, train_shape, test_shape, strat, embed_method in get_graph_datasets(DS_BATCH_SIZE, db_shapes)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[(<__main__.BatchedProgramDataset at 0x7fafa6467760>,\n  <__main__.BatchedProgramDataset at 0x7fafa64655a0>,\n  (282, 4, 29, 50),\n  (70, 4, 29, 50),\n  'FC',\n  'deepwalk'),\n (<__main__.BatchedProgramDataset at 0x7fafa64653c0>,\n  <__main__.BatchedProgramDataset at 0x7fafa64652d0>,\n  (273, 4, 29, 50),\n  (79, 4, 29, 50),\n  'VC',\n  'deepwalk'),\n (<__main__.BatchedProgramDataset at 0x7fafa6465390>,\n  <__main__.BatchedProgramDataset at 0x7fafa64651e0>,\n  (284, 4, 29, 50),\n  (68, 4, 29, 50),\n  'LC',\n  'deepwalk'),\n (<__main__.BatchedProgramDataset at 0x7fafa6465300>,\n  <__main__.BatchedProgramDataset at 0x7fafa64651b0>,\n  (273, 4, 29, 50),\n  (79, 4, 29, 50),\n  'NC',\n  'deepwalk'),\n (<__main__.BatchedProgramDataset at 0x7fafa6465360>,\n  <__main__.BatchedProgramDataset at 0x7fafa6465210>,\n  (282, 4, 29, 50),\n  (70, 4, 29, 50),\n  'FC',\n  'node2vec'),\n (<__main__.BatchedProgramDataset at 0x7fafa6464cd0>,\n  <__main__.BatchedProgramDataset at 0x7fafa6467820>,\n  (273, 4, 29, 50),\n  (79, 4, 29, 50),\n  'VC',\n  'node2vec'),\n (<__main__.BatchedProgramDataset at 0x7fafa6467850>,\n  <__main__.BatchedProgramDataset at 0x7fafa6467190>,\n  (284, 4, 29, 50),\n  (68, 4, 29, 50),\n  'LC',\n  'node2vec'),\n (<__main__.BatchedProgramDataset at 0x7fafa6467160>,\n  <__main__.BatchedProgramDataset at 0x7fafa64670d0>,\n  (273, 4, 29, 50),\n  (79, 4, 29, 50),\n  'NC',\n  'node2vec')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[0][0])\n",
    "datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:0. Training FC-deepwalk prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training VC-deepwalk prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training LC-deepwalk prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training NC-deepwalk prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training FC-node2vec prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training VC-node2vec prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training LC-node2vec prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:0. Training NC-node2vec prediction model w/5 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training FC-deepwalk prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training VC-deepwalk prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training LC-deepwalk prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training NC-deepwalk prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training FC-node2vec prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training VC-node2vec prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training LC-node2vec prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:1. Training NC-node2vec prediction model w/5 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training FC-deepwalk prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training VC-deepwalk prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training LC-deepwalk prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training NC-deepwalk prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training FC-node2vec prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training VC-node2vec prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training LC-node2vec prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:2. Training NC-node2vec prediction model w/5 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training FC-deepwalk prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training VC-deepwalk prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training LC-deepwalk prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training NC-deepwalk prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training FC-node2vec prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training VC-node2vec prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training LC-node2vec prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:3. Training NC-node2vec prediction model w/5 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training FC-deepwalk prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training VC-deepwalk prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training LC-deepwalk prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training NC-deepwalk prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training FC-node2vec prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training VC-node2vec prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training LC-node2vec prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:4. Training NC-node2vec prediction model w/5 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training FC-deepwalk prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training VC-deepwalk prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training LC-deepwalk prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training NC-deepwalk prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training FC-node2vec prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training VC-node2vec prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training LC-node2vec prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:5. Training NC-node2vec prediction model w/10 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training FC-deepwalk prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training VC-deepwalk prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training LC-deepwalk prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training NC-deepwalk prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training FC-node2vec prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training VC-node2vec prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training LC-node2vec prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:6. Training NC-node2vec prediction model w/10 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training FC-deepwalk prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training VC-deepwalk prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training LC-deepwalk prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training NC-deepwalk prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training FC-node2vec prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training VC-node2vec prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training LC-node2vec prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:7. Training NC-node2vec prediction model w/10 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training FC-deepwalk prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training VC-deepwalk prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training LC-deepwalk prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training NC-deepwalk prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training FC-node2vec prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training VC-node2vec prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training LC-node2vec prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:8. Training NC-node2vec prediction model w/10 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training FC-deepwalk prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training VC-deepwalk prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training LC-deepwalk prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training NC-deepwalk prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training FC-node2vec prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training VC-node2vec prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training LC-node2vec prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:9. Training NC-node2vec prediction model w/10 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training FC-deepwalk prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training VC-deepwalk prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training LC-deepwalk prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training NC-deepwalk prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training FC-node2vec prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training VC-node2vec prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training LC-node2vec prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:10. Training NC-node2vec prediction model w/50 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training FC-deepwalk prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training VC-deepwalk prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training LC-deepwalk prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training NC-deepwalk prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training FC-node2vec prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training VC-node2vec prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training LC-node2vec prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:11. Training NC-node2vec prediction model w/50 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training FC-deepwalk prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training VC-deepwalk prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training LC-deepwalk prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training NC-deepwalk prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training FC-node2vec prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training VC-node2vec prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training LC-node2vec prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:12. Training NC-node2vec prediction model w/50 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training FC-deepwalk prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training VC-deepwalk prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training LC-deepwalk prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training NC-deepwalk prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training FC-node2vec prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training VC-node2vec prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training LC-node2vec prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:13. Training NC-node2vec prediction model w/50 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training FC-deepwalk prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training VC-deepwalk prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training LC-deepwalk prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training NC-deepwalk prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training FC-node2vec prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training VC-node2vec prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training LC-node2vec prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:14. Training NC-node2vec prediction model w/50 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training FC-deepwalk prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training VC-deepwalk prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training LC-deepwalk prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training NC-deepwalk prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training FC-node2vec prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training VC-node2vec prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training LC-node2vec prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:15. Training NC-node2vec prediction model w/100 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training FC-deepwalk prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training VC-deepwalk prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training LC-deepwalk prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training NC-deepwalk prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training FC-node2vec prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training VC-node2vec prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training LC-node2vec prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:16. Training NC-node2vec prediction model w/100 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training FC-deepwalk prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training VC-deepwalk prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training LC-deepwalk prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training NC-deepwalk prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training FC-node2vec prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training VC-node2vec prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training LC-node2vec prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:17. Training NC-node2vec prediction model w/100 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training FC-deepwalk prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training VC-deepwalk prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training LC-deepwalk prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training NC-deepwalk prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training FC-node2vec prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training VC-node2vec prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training LC-node2vec prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:18. Training NC-node2vec prediction model w/100 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training FC-deepwalk prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training VC-deepwalk prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training LC-deepwalk prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training NC-deepwalk prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training FC-node2vec prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training VC-node2vec prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training LC-node2vec prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:19. Training NC-node2vec prediction model w/100 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training FC-deepwalk prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training VC-deepwalk prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training LC-deepwalk prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training NC-deepwalk prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training FC-node2vec prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training VC-node2vec prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training LC-node2vec prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:20. Training NC-node2vec prediction model w/500 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training FC-deepwalk prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training VC-deepwalk prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training LC-deepwalk prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training NC-deepwalk prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training FC-node2vec prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training VC-node2vec prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training LC-node2vec prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:21. Training NC-node2vec prediction model w/500 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training FC-deepwalk prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training VC-deepwalk prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training LC-deepwalk prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training NC-deepwalk prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training FC-node2vec prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training VC-node2vec prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training LC-node2vec prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:22. Training NC-node2vec prediction model w/500 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training FC-deepwalk prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training VC-deepwalk prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training LC-deepwalk prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training NC-deepwalk prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training FC-node2vec prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training VC-node2vec prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training LC-node2vec prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:23. Training NC-node2vec prediction model w/500 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training FC-deepwalk prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training VC-deepwalk prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training LC-deepwalk prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training NC-deepwalk prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training FC-node2vec prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training VC-node2vec prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training LC-node2vec prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:24. Training NC-node2vec prediction model w/500 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training FC-deepwalk prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training VC-deepwalk prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training LC-deepwalk prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training NC-deepwalk prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training FC-node2vec prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training VC-node2vec prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training LC-node2vec prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:25. Training NC-node2vec prediction model w/1000 epochs and a 5 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training FC-deepwalk prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training VC-deepwalk prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training LC-deepwalk prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training NC-deepwalk prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training FC-node2vec prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training VC-node2vec prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training LC-node2vec prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:26. Training NC-node2vec prediction model w/1000 epochs and a 1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training FC-deepwalk prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training VC-deepwalk prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training LC-deepwalk prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training NC-deepwalk prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training FC-node2vec prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training VC-node2vec prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training LC-node2vec prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:27. Training NC-node2vec prediction model w/1000 epochs and a 0.1 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training FC-deepwalk prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training VC-deepwalk prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training LC-deepwalk prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training NC-deepwalk prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training FC-node2vec prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training VC-node2vec prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training LC-node2vec prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:28. Training NC-node2vec prediction model w/1000 epochs and a 0.01 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training FC-deepwalk prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training VC-deepwalk prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training LC-deepwalk prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training NC-deepwalk prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training FC-node2vec prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training VC-node2vec prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training LC-node2vec prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n",
      "CRITICAL:root:29. Training NC-node2vec prediction model w/1000 epochs and a 1e-05 learning rate\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 5, 5, 'FC-deepwalk avg loss'): 0.4711681604385376,\n",
      " (0, 5, 5, 'FC-node2vec avg loss'): 0.4733625253041585,\n",
      " (0, 5, 5, 'LC-deepwalk avg loss'): 0.5048283239205679,\n",
      " (0, 5, 5, 'LC-node2vec avg loss'): 0.5062294900417328,\n",
      " (0, 5, 5, 'NC-deepwalk avg loss'): 0.4556235869725545,\n",
      " (0, 5, 5, 'NC-node2vec avg loss'): 0.4549266993999481,\n",
      " (0, 5, 5, 'VC-deepwalk avg loss'): 0.4973299602667491,\n",
      " (0, 5, 5, 'VC-node2vec avg loss'): 0.49885615706443787,\n",
      " (1, 5, 1, 'FC-deepwalk avg loss'): 0.4712962806224823,\n",
      " (1, 5, 1, 'FC-node2vec avg loss'): 0.4885110358397166,\n",
      " (1, 5, 1, 'LC-deepwalk avg loss'): 0.5046083430449168,\n",
      " (1, 5, 1, 'LC-node2vec avg loss'): 0.5192470848560333,\n",
      " (1, 5, 1, 'NC-deepwalk avg loss'): 0.45274584492047626,\n",
      " (1, 5, 1, 'NC-node2vec avg loss'): 0.47166235248247784,\n",
      " (1, 5, 1, 'VC-deepwalk avg loss'): 0.4977423946062724,\n",
      " (1, 5, 1, 'VC-node2vec avg loss'): 0.5144098500410715,\n",
      " (2, 5, 0.1, 'FC-deepwalk avg loss'): 0.48950116833051044,\n",
      " (2, 5, 0.1, 'FC-node2vec avg loss'): 0.4948656956354777,\n",
      " (2, 5, 0.1, 'LC-deepwalk avg loss'): 0.5143138070901235,\n",
      " (2, 5, 0.1, 'LC-node2vec avg loss'): 0.5250906348228455,\n",
      " (2, 5, 0.1, 'NC-deepwalk avg loss'): 0.47776564955711365,\n",
      " (2, 5, 0.1, 'NC-node2vec avg loss'): 0.4778860608736674,\n",
      " (2, 5, 0.1, 'VC-deepwalk avg loss'): 0.5162630081176758,\n",
      " (2, 5, 0.1, 'VC-node2vec avg loss'): 0.5188190241654714,\n",
      " (3, 5, 0.01, 'FC-deepwalk avg loss'): 0.497721532980601,\n",
      " (3, 5, 0.01, 'FC-node2vec avg loss'): 0.49648432930310565,\n",
      " (3, 5, 0.01, 'LC-deepwalk avg loss'): 0.5251389145851135,\n",
      " (3, 5, 0.01, 'LC-node2vec avg loss'): 0.524813731511434,\n",
      " (3, 5, 0.01, 'NC-deepwalk avg loss'): 0.4786876142024994,\n",
      " (3, 5, 0.01, 'NC-node2vec avg loss'): 0.4781128168106079,\n",
      " (3, 5, 0.01, 'VC-deepwalk avg loss'): 0.5193759401639303,\n",
      " (3, 5, 0.01, 'VC-node2vec avg loss'): 0.5188172856966654,\n",
      " (4, 5, 1e-05, 'FC-deepwalk avg loss'): 0.4964398841063182,\n",
      " (4, 5, 1e-05, 'FC-node2vec avg loss'): 0.4974479079246521,\n",
      " (4, 5, 1e-05, 'LC-deepwalk avg loss'): 0.5250825881958008,\n",
      " (4, 5, 1e-05, 'LC-node2vec avg loss'): 0.5254739125569662,\n",
      " (4, 5, 1e-05, 'NC-deepwalk avg loss'): 0.4786749680836995,\n",
      " (4, 5, 1e-05, 'NC-node2vec avg loss'): 0.4778853853543599,\n",
      " (4, 5, 1e-05, 'VC-deepwalk avg loss'): 0.5181500017642975,\n",
      " (4, 5, 1e-05, 'VC-node2vec avg loss'): 0.5196252465248108,\n",
      " (5, 10, 5, 'FC-deepwalk avg loss'): 0.47112510601679486,\n",
      " (5, 10, 5, 'FC-node2vec avg loss'): 0.4722713530063629,\n",
      " (5, 10, 5, 'LC-deepwalk avg loss'): 0.504336287577947,\n",
      " (5, 10, 5, 'LC-node2vec avg loss'): 0.5052026907602946,\n",
      " (5, 10, 5, 'NC-deepwalk avg loss'): 0.4513838092486064,\n",
      " (5, 10, 5, 'NC-node2vec avg loss'): 0.4528172512849172,\n",
      " (5, 10, 5, 'VC-deepwalk avg loss'): 0.4973650773366292,\n",
      " (5, 10, 5, 'VC-node2vec avg loss'): 0.49783217906951904,\n",
      " (6, 10, 1, 'FC-deepwalk avg loss'): 0.4711007575194041,\n",
      " (6, 10, 1, 'FC-node2vec avg loss'): 0.4838212927182515,\n",
      " (6, 10, 1, 'LC-deepwalk avg loss'): 0.5044646461804708,\n",
      " (6, 10, 1, 'LC-node2vec avg loss'): 0.5154216984907786,\n",
      " (6, 10, 1, 'NC-deepwalk avg loss'): 0.4526981512705485,\n",
      " (6, 10, 1, 'NC-node2vec avg loss'): 0.46751434604326886,\n",
      " (6, 10, 1, 'VC-deepwalk avg loss'): 0.4975992838541667,\n",
      " (6, 10, 1, 'VC-node2vec avg loss'): 0.5068046847979227,\n",
      " (7, 10, 0.1, 'FC-deepwalk avg loss'): 0.47388457258542377,\n",
      " (7, 10, 0.1, 'FC-node2vec avg loss'): 0.49641619126001996,\n",
      " (7, 10, 0.1, 'LC-deepwalk avg loss'): 0.52164359887441,\n",
      " (7, 10, 0.1, 'LC-node2vec avg loss'): 0.5237256089846293,\n",
      " (7, 10, 0.1, 'NC-deepwalk avg loss'): 0.47690293192863464,\n",
      " (7, 10, 0.1, 'NC-node2vec avg loss'): 0.4769875804583232,\n",
      " (7, 10, 0.1, 'VC-deepwalk avg loss'): 0.5160493354002634,\n",
      " (7, 10, 0.1, 'VC-node2vec avg loss'): 0.5164022048314413,\n",
      " (8, 10, 0.01, 'FC-deepwalk avg loss'): 0.49663318196932477,\n",
      " (8, 10, 0.01, 'FC-node2vec avg loss'): 0.4967859784762065,\n",
      " (8, 10, 0.01, 'LC-deepwalk avg loss'): 0.5243754982948303,\n",
      " (8, 10, 0.01, 'LC-node2vec avg loss'): 0.5245047807693481,\n",
      " (8, 10, 0.01, 'NC-deepwalk avg loss'): 0.47827579577763873,\n",
      " (8, 10, 0.01, 'NC-node2vec avg loss'): 0.4785686731338501,\n",
      " (8, 10, 0.01, 'VC-deepwalk avg loss'): 0.5181205570697784,\n",
      " (8, 10, 0.01, 'VC-node2vec avg loss'): 0.5178297857443491,\n",
      " (9, 10, 1e-05, 'FC-deepwalk avg loss'): 0.49643515547116596,\n",
      " (9, 10, 1e-05, 'FC-node2vec avg loss'): 0.4975365897019704,\n",
      " (9, 10, 1e-05, 'LC-deepwalk avg loss'): 0.5240358114242554,\n",
      " (9, 10, 1e-05, 'LC-node2vec avg loss'): 0.5259880622227987,\n",
      " (9, 10, 1e-05, 'NC-deepwalk avg loss'): 0.4790523052215576,\n",
      " (9, 10, 1e-05, 'NC-node2vec avg loss'): 0.477718045314153,\n",
      " (9, 10, 1e-05, 'VC-deepwalk avg loss'): 0.5179561674594879,\n",
      " (9, 10, 1e-05, 'VC-node2vec avg loss'): 0.5179040332635244,\n",
      " (10, 50, 5, 'FC-deepwalk avg loss'): 0.4617832601070404,\n",
      " (10, 50, 5, 'FC-node2vec avg loss'): 0.4718569914499919,\n",
      " (10, 50, 5, 'LC-deepwalk avg loss'): 0.4887500007947286,\n",
      " (10, 50, 5, 'LC-node2vec avg loss'): 0.5049386521180471,\n",
      " (10, 50, 5, 'NC-deepwalk avg loss'): 0.44668568174044293,\n",
      " (10, 50, 5, 'NC-node2vec avg loss'): 0.4523654381434123,\n",
      " (10, 50, 5, 'VC-deepwalk avg loss'): 0.4939152002334595,\n",
      " (10, 50, 5, 'VC-node2vec avg loss'): 0.49753032128016156,\n",
      " (11, 50, 1, 'FC-deepwalk avg loss'): 0.46753443280855816,\n",
      " (11, 50, 1, 'FC-node2vec avg loss'): 0.47240035732587177,\n",
      " (11, 50, 1, 'LC-deepwalk avg loss'): 0.4965486029783885,\n",
      " (11, 50, 1, 'LC-node2vec avg loss'): 0.5052831570307413,\n",
      " (11, 50, 1, 'NC-deepwalk avg loss'): 0.45234684149424237,\n",
      " (11, 50, 1, 'NC-node2vec avg loss'): 0.4526226421197255,\n",
      " (11, 50, 1, 'VC-deepwalk avg loss'): 0.49796732266743976,\n",
      " (11, 50, 1, 'VC-node2vec avg loss'): 0.49781657258669537,\n",
      " (12, 50, 0.1, 'FC-deepwalk avg loss'): 0.4718855818112691,\n",
      " (12, 50, 0.1, 'FC-node2vec avg loss'): 0.490672489007314,\n",
      " (12, 50, 0.1, 'LC-deepwalk avg loss'): 0.5043687125047048,\n",
      " (12, 50, 0.1, 'LC-node2vec avg loss'): 0.5186520417531332,\n",
      " (12, 50, 0.1, 'NC-deepwalk avg loss'): 0.4523984690507253,\n",
      " (12, 50, 0.1, 'NC-node2vec avg loss'): 0.4724625150362651,\n",
      " (12, 50, 0.1, 'VC-deepwalk avg loss'): 0.497693806886673,\n",
      " (12, 50, 0.1, 'VC-node2vec avg loss'): 0.512442280848821,\n",
      " (13, 50, 0.01, 'FC-deepwalk avg loss'): 0.49554497996966046,\n",
      " (13, 50, 0.01, 'FC-node2vec avg loss'): 0.49631695946057636,\n",
      " (13, 50, 0.01, 'LC-deepwalk avg loss'): 0.5221625367800394,\n",
      " (13, 50, 0.01, 'LC-node2vec avg loss'): 0.5250487128893534,\n",
      " (13, 50, 0.01, 'NC-deepwalk avg loss'): 0.4791513780752818,\n",
      " (13, 50, 0.01, 'NC-node2vec avg loss'): 0.4786846538384755,\n",
      " (13, 50, 0.01, 'VC-deepwalk avg loss'): 0.5179920097192129,\n",
      " (13, 50, 0.01, 'VC-node2vec avg loss'): 0.5173666874567667,\n",
      " (14, 50, 1e-05, 'FC-deepwalk avg loss'): 0.4954537053902944,\n",
      " (14, 50, 1e-05, 'FC-node2vec avg loss'): 0.4959928095340729,\n",
      " (14, 50, 1e-05, 'LC-deepwalk avg loss'): 0.5243977705637614,\n",
      " (14, 50, 1e-05, 'LC-node2vec avg loss'): 0.5247548222541809,\n",
      " (14, 50, 1e-05, 'NC-deepwalk avg loss'): 0.479099581638972,\n",
      " (14, 50, 1e-05, 'NC-node2vec avg loss'): 0.4767446716626485,\n",
      " (14, 50, 1e-05, 'VC-deepwalk avg loss'): 0.517585963010788,\n",
      " (14, 50, 1e-05, 'VC-node2vec avg loss'): 0.5193352103233337,\n",
      " (15, 100, 5, 'FC-deepwalk avg loss'): 0.46597134073575336,\n",
      " (15, 100, 5, 'FC-node2vec avg loss'): 0.4718390901883443,\n",
      " (15, 100, 5, 'LC-deepwalk avg loss'): 0.48886282245318097,\n",
      " (15, 100, 5, 'LC-node2vec avg loss'): 0.5049184660116831,\n",
      " (15, 100, 5, 'NC-deepwalk avg loss'): 0.4458228846391042,\n",
      " (15, 100, 5, 'NC-node2vec avg loss'): 0.4523552854855855,\n",
      " (15, 100, 5, 'VC-deepwalk avg loss'): 0.491907795270284,\n",
      " (15, 100, 5, 'VC-node2vec avg loss'): 0.49750666817029315,\n",
      " (16, 100, 1, 'FC-deepwalk avg loss'): 0.4659205873807271,\n",
      " (16, 100, 1, 'FC-node2vec avg loss'): 0.4719909429550171,\n",
      " (16, 100, 1, 'LC-deepwalk avg loss'): 0.4936728278795878,\n",
      " (16, 100, 1, 'LC-node2vec avg loss'): 0.5050453046957651,\n",
      " (16, 100, 1, 'NC-deepwalk avg loss'): 0.451105535030365,\n",
      " (16, 100, 1, 'NC-node2vec avg loss'): 0.4523131151994069,\n",
      " (16, 100, 1, 'VC-deepwalk avg loss'): 0.4926501413186391,\n",
      " (16, 100, 1, 'VC-node2vec avg loss'): 0.4976072708765666,\n",
      " (17, 100, 0.1, 'FC-deepwalk avg loss'): 0.4712928632895152,\n",
      " (17, 100, 0.1, 'FC-node2vec avg loss'): 0.48630833625793457,\n",
      " (17, 100, 0.1, 'LC-deepwalk avg loss'): 0.5031273066997528,\n",
      " (17, 100, 0.1, 'LC-node2vec avg loss'): 0.5129694640636444,\n",
      " (17, 100, 0.1, 'NC-deepwalk avg loss'): 0.4523881872495015,\n",
      " (17, 100, 0.1, 'NC-node2vec avg loss'): 0.4678712785243988,\n",
      " (17, 100, 0.1, 'VC-deepwalk avg loss'): 0.49754956364631653,\n",
      " (17, 100, 0.1, 'VC-node2vec avg loss'): 0.5068705379962921,\n",
      " (18, 100, 0.01, 'FC-deepwalk avg loss'): 0.4918615221977234,\n",
      " (18, 100, 0.01, 'FC-node2vec avg loss'): 0.4953739047050476,\n",
      " (18, 100, 0.01, 'LC-deepwalk avg loss'): 0.5052691797415415,\n",
      " (18, 100, 0.01, 'LC-node2vec avg loss'): 0.5241466363271078,\n",
      " (18, 100, 0.01, 'NC-deepwalk avg loss'): 0.4776151378949483,\n",
      " (18, 100, 0.01, 'NC-node2vec avg loss'): 0.47758714358011883,\n",
      " (18, 100, 0.01, 'VC-deepwalk avg loss'): 0.5158290465672811,\n",
      " (18, 100, 0.01, 'VC-node2vec avg loss'): 0.5175590813159943,\n",
      " (19, 100, 1e-05, 'FC-deepwalk avg loss'): 0.4964517156283061,\n",
      " (19, 100, 1e-05, 'FC-node2vec avg loss'): 0.49689961473147076,\n",
      " (19, 100, 1e-05, 'LC-deepwalk avg loss'): 0.5239196221033732,\n",
      " (19, 100, 1e-05, 'LC-node2vec avg loss'): 0.5246565739313761,\n",
      " (19, 100, 1e-05, 'NC-deepwalk avg loss'): 0.47892944018046063,\n",
      " (19, 100, 1e-05, 'NC-node2vec avg loss'): 0.478887677192688,\n",
      " (19, 100, 1e-05, 'VC-deepwalk avg loss'): 0.5170870224634806,\n",
      " (19, 100, 1e-05, 'VC-node2vec avg loss'): 0.5181796153386434,\n",
      " (20, 500, 5, 'FC-deepwalk avg loss'): 0.4650460481643677,\n",
      " (20, 500, 5, 'FC-node2vec avg loss'): 0.47182759642601013,\n",
      " (20, 500, 5, 'LC-deepwalk avg loss'): 0.49102460344632465,\n",
      " (20, 500, 5, 'LC-node2vec avg loss'): 0.5048997402191162,\n",
      " (20, 500, 5, 'NC-deepwalk avg loss'): 0.44475797812143963,\n",
      " (20, 500, 5, 'NC-node2vec avg loss'): 0.4523227910200755,\n",
      " (20, 500, 5, 'VC-deepwalk avg loss'): 0.48973706364631653,\n",
      " (20, 500, 5, 'VC-node2vec avg loss'): 0.49749041597048443,\n",
      " (21, 500, 1, 'FC-deepwalk avg loss'): 0.46430835127830505,\n",
      " (21, 500, 1, 'FC-node2vec avg loss'): 0.4718593955039978,\n",
      " (21, 500, 1, 'LC-deepwalk avg loss'): 0.4964413543542226,\n",
      " (21, 500, 1, 'LC-node2vec avg loss'): 0.5049263536930084,\n",
      " (21, 500, 1, 'NC-deepwalk avg loss'): 0.44748153289159137,\n",
      " (21, 500, 1, 'NC-node2vec avg loss'): 0.4521968563397725,\n",
      " (21, 500, 1, 'VC-deepwalk avg loss'): 0.49375369151433307,\n",
      " (21, 500, 1, 'VC-node2vec avg loss'): 0.49749061465263367,\n",
      " (22, 500, 0.1, 'FC-deepwalk avg loss'): 0.46649425228436786,\n",
      " (22, 500, 0.1, 'FC-node2vec avg loss'): 0.4722979962825775,\n",
      " (22, 500, 0.1, 'LC-deepwalk avg loss'): 0.4992050627867381,\n",
      " (22, 500, 0.1, 'LC-node2vec avg loss'): 0.5053206185499827,\n",
      " (22, 500, 0.1, 'NC-deepwalk avg loss'): 0.45096168915430707,\n",
      " (22, 500, 0.1, 'NC-node2vec avg loss'): 0.45262861251831055,\n",
      " (22, 500, 0.1, 'VC-deepwalk avg loss'): 0.49755018949508667,\n",
      " (22, 500, 0.1, 'VC-node2vec avg loss'): 0.4978616038958232,\n",
      " (23, 500, 0.01, 'FC-deepwalk avg loss'): 0.4718627631664276,\n",
      " (23, 500, 0.01, 'FC-node2vec avg loss'): 0.4899381101131439,\n",
      " (23, 500, 0.01, 'LC-deepwalk avg loss'): 0.5043540696303049,\n",
      " (23, 500, 0.01, 'LC-node2vec avg loss'): 0.5198187033335367,\n",
      " (23, 500, 0.01, 'NC-deepwalk avg loss'): 0.45222216844558716,\n",
      " (23, 500, 0.01, 'NC-node2vec avg loss'): 0.4711101750532786,\n",
      " (23, 500, 0.01, 'VC-deepwalk avg loss'): 0.4976476728916168,\n",
      " (23, 500, 0.01, 'VC-node2vec avg loss'): 0.5136768221855164,\n",
      " (24, 500, 1e-05, 'FC-deepwalk avg loss'): 0.49646226565043133,\n",
      " (24, 500, 1e-05, 'FC-node2vec avg loss'): 0.49623868862787884,\n",
      " (24, 500, 1e-05, 'LC-deepwalk avg loss'): 0.5253439942995707,\n",
      " (24, 500, 1e-05, 'LC-node2vec avg loss'): 0.5248520175615946,\n",
      " (24, 500, 1e-05, 'NC-deepwalk avg loss'): 0.4780244131882985,\n",
      " (24, 500, 1e-05, 'NC-node2vec avg loss'): 0.47695021828015643,\n",
      " (24, 500, 1e-05, 'VC-deepwalk avg loss'): 0.5188896556695303,\n",
      " (24, 500, 1e-05, 'VC-node2vec avg loss'): 0.5192320545514425,\n",
      " (25, 1000, 5, 'FC-deepwalk avg loss'): 0.46075068910916644,\n",
      " (25, 1000, 5, 'FC-node2vec avg loss'): 0.4718259572982788,\n",
      " (25, 1000, 5, 'LC-deepwalk avg loss'): 0.4942229092121124,\n",
      " (25, 1000, 5, 'LC-node2vec avg loss'): 0.5048977533976237,\n",
      " (25, 1000, 5, 'NC-deepwalk avg loss'): 0.4430168569087982,\n",
      " (25, 1000, 5, 'NC-node2vec avg loss'): 0.45228848854700726,\n",
      " (25, 1000, 5, 'VC-deepwalk avg loss'): 0.4878317316373189,\n",
      " (25, 1000, 5, 'VC-node2vec avg loss'): 0.4974871178468068,\n",
      " (26, 1000, 1, 'FC-deepwalk avg loss'): 0.46453248461087543,\n",
      " (26, 1000, 1, 'FC-node2vec avg loss'): 0.47185375293095905,\n",
      " (26, 1000, 1, 'LC-deepwalk avg loss'): 0.49172285199165344,\n",
      " (26, 1000, 1, 'LC-node2vec avg loss'): 0.5049156546592712,\n",
      " (26, 1000, 1, 'NC-deepwalk avg loss'): 0.4488711357116699,\n",
      " (26, 1000, 1, 'NC-node2vec avg loss'): 0.4521910349527995,\n",
      " (26, 1000, 1, 'VC-deepwalk avg loss'): 0.4916634162267049,\n",
      " (26, 1000, 1, 'VC-node2vec avg loss'): 0.4974788725376129,\n",
      " (27, 1000, 0.1, 'FC-deepwalk avg loss'): 0.46531962354977924,\n",
      " (27, 1000, 0.1, 'FC-node2vec avg loss'): 0.47202448050181073,\n",
      " (27, 1000, 0.1, 'LC-deepwalk avg loss'): 0.49729634324709576,\n",
      " (27, 1000, 0.1, 'LC-node2vec avg loss'): 0.505028227965037,\n",
      " (27, 1000, 0.1, 'NC-deepwalk avg loss'): 0.45123640696207684,\n",
      " (27, 1000, 0.1, 'NC-node2vec avg loss'): 0.4522581696510315,\n",
      " (27, 1000, 0.1, 'VC-deepwalk avg loss'): 0.49600786964098614,\n",
      " (27, 1000, 0.1, 'VC-node2vec avg loss'): 0.4976181387901306,\n",
      " (28, 1000, 0.01, 'FC-deepwalk avg loss'): 0.4713699320952098,\n",
      " (28, 1000, 0.01, 'FC-node2vec avg loss'): 0.48185192545255023,\n",
      " (28, 1000, 0.01, 'LC-deepwalk avg loss'): 0.5029331048329672,\n",
      " (28, 1000, 0.01, 'LC-node2vec avg loss'): 0.513859768708547,\n",
      " (28, 1000, 0.01, 'NC-deepwalk avg loss'): 0.4522746205329895,\n",
      " (28, 1000, 0.01, 'NC-node2vec avg loss'): 0.46543153127034503,\n",
      " (28, 1000, 0.01, 'VC-deepwalk avg loss'): 0.4975589116414388,\n",
      " (28, 1000, 0.01, 'VC-node2vec avg loss'): 0.5097202757994334,\n",
      " (29, 1000, 1e-05, 'FC-deepwalk avg loss'): 0.4978252351284027,\n",
      " (29, 1000, 1e-05, 'FC-node2vec avg loss'): 0.4969284435113271,\n",
      " (29, 1000, 1e-05, 'LC-deepwalk avg loss'): 0.525510311126709,\n",
      " (29, 1000, 1e-05, 'LC-node2vec avg loss'): 0.5246556798617045,\n",
      " (29, 1000, 1e-05, 'NC-deepwalk avg loss'): 0.47825825214385986,\n",
      " (29, 1000, 1e-05, 'NC-node2vec avg loss'): 0.4783242742220561,\n",
      " (29, 1000, 1e-05, 'VC-deepwalk avg loss'): 0.5176499883333842,\n",
      " (29, 1000, 1e-05, 'VC-node2vec avg loss'): 0.5194229980309805}\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel('CRITICAL')\n",
    "\n",
    "pth_model_dir = Path.cwd() / 'models'\n",
    "\n",
    "import math\n",
    "\n",
    "def train_loop(dataset, model, loss_function, optimizer):\n",
    "    size = len(dataset)\n",
    "    batch = 0\n",
    "    input_tensor = None\n",
    "    loss = None\n",
    "    for X, y in torch.utils.data.DataLoader(dataset, num_workers = 0):\n",
    "        pred = model(X)\n",
    "        logging.debug(pred)\n",
    "        logging.debug(y)\n",
    "        loss = loss_function(pred.squeeze(), y.squeeze())\n",
    "        input_tensor = X\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logging.debug(loss)\n",
    "        if batch % 10 == 0:\n",
    "            batch_loss, current = loss.item(), batch * len(X)\n",
    "            logging.info(f'loss: {batch_loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "        batch += 1\n",
    "\n",
    "    loss, current = loss.item(), batch * len(input_tensor)\n",
    "    logging.info(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def val_loop(dataset, model, loss_function):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, num_workers = 0)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    sig = nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            pred = sig(pred)\n",
    "            test_loss += loss_function(pred.squeeze(), y.squeeze()).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    logging.info(f'Test Error: \\nAvg loss: {test_loss:>8f}\\n')\n",
    "    return test_loss\n",
    "\n",
    "epochs_list = [5, 10, 50, 100, 500, 1000]\n",
    "learning_rates = [5, 1, 1e-1, 1e-2, 1e-5]\n",
    "final_scores = {}\n",
    "for id, (epochs, lr) in enumerate(itertools.product(epochs_list, learning_rates)):\n",
    "    for train, test, train_shape, test_shape, strat, embed_method in datasets:\n",
    "        logging.critical(f'{id}. Training {strat}-{embed_method} prediction model w/{epochs} epochs and a {lr} learning rate\\n------------------------------')\n",
    "        model = CfgMisconceptionClassifier(50).to(device)\n",
    "        loss_function = torch.nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "        train = list(train)\n",
    "        train_len = math.floor(len(train) * .875)\n",
    "        val_len = len(train) - train_len\n",
    "        for t in range(epochs):\n",
    "            logging.info(f'Epoch {t}\\n------------------------------')\n",
    "            train_ds, val = torch.utils.data.random_split(train, [train_len, val_len], generator=torch.Generator().manual_seed(train_len))\n",
    "            train_loop(train_ds, model, loss_function, optimizer)\n",
    "            val_loss = val_loop(val, model, loss_function)\n",
    "\n",
    "        final_scores[(id, epochs, lr, f'{strat}-{embed_method} avg loss')] = val_loop(test, model, loss_function)\n",
    "        torch.save(model.state_dict(), pth_model_dir / f'{strat}_{embed_method}_e{epochs}_lr{lr:e}_{id}_sigmoid_mi_weights.pth')\n",
    "        del model\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(final_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(final_scores.items(), key=lambda item: item[1])}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def test_loop(dataset, model, loss_function, most_common_class):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, num_workers = 0)\n",
    "    num_batches = len(dataloader)\n",
    "    total_programs = num_batches * dataset.batch_size\n",
    "    test_loss = 0\n",
    "    c_test_loss = 0\n",
    "    counts = []\n",
    "    c_counts = []\n",
    "    mcc_tensor = torch.as_tensor(most_common_class).to(device)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            full_prediction = np.around(pred.squeeze().cpu().numpy())\n",
    "            actual = y.squeeze().cpu().numpy()\n",
    "            counts.append(np.count_nonzero(np.equal(full_prediction, actual), axis=0))\n",
    "            c_counts.append(np.count_nonzero(np.equal(most_common_class, actual), axis=0))\n",
    "            test_loss += loss_function(pred.squeeze(), y.squeeze()).item()\n",
    "            c_test_loss += loss_function(mcc_tensor, y.squeeze()).item()\n",
    "\n",
    "    count_arr = np.array(counts)\n",
    "    count_arr = count_arr.sum(axis=0, dtype=np.float32)\n",
    "    count_arr /= float(total_programs)\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    c_count_arr = np.array(c_counts)\n",
    "    c_count_arr = c_count_arr.sum(axis=0, dtype=np.float32)\n",
    "    c_count_arr /= float(total_programs)\n",
    "    c_test_loss /= num_batches\n",
    "    logging.info(f'Test Error: \\nAvg loss: {test_loss:>8f}\\n')\n",
    "    return np.append(count_arr, test_loss), np.append(c_count_arr, c_test_loss)\n",
    "\n",
    "epochs_list = [5, 10, 50, 100, 500, 1000]\n",
    "learning_rates = [5, 1, 1e-1, 1e-2, 1e-5]\n",
    "final_scores = {}\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "ncs = [ds for ds in datasets if ds[4] == 'NC']\n",
    "data = []\n",
    "columns = []\n",
    "for train, test, train_shape, test_shape, strat, embed_method in ncs:\n",
    "    for id, (epochs, lr) in enumerate(itertools.product(epochs_list, learning_rates)):\n",
    "        model = CfgMisconceptionClassifier(50).to(device)\n",
    "        model_dict = torch.load(pth_model_dir / f'{strat}_{embed_method}_e{epochs}_lr{lr:e}_{id}_sigmoid_mi_weights.pth')\n",
    "        model.load_state_dict(model_dict)\n",
    "        loss_function = torch.nn.MultiLabelSoftMarginLoss()\n",
    "        pred_result, common_result = test_loop(test, model, loss_function, np.ones((test.batch_size, 11)))\n",
    "        data.append(pred_result)\n",
    "        columns.append(f'{strat}-{embed_method}-E{epochs}-LR{lr}-{id}')\n",
    "\n",
    "nc_results = pd.DataFrame(data=data + [common_result], columns=tags_titles + ['Loss'], index=columns + ['Assume Most Common Class'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "                              Accuracy      Loss\nNC-deepwalk-E5-LR5-0          0.780556  0.416198\nNC-deepwalk-E5-LR1-1          0.844444  0.400739\nNC-deepwalk-E5-LR0.1-2        0.550000  0.509405\nNC-deepwalk-E5-LR0.01-3       0.468056  0.513663\nNC-deepwalk-E5-LR1e-05-4      0.516667  0.513796\n...                                ...       ...\nNC-node2vec-E1000-LR1-26      0.844444  0.398353\nNC-node2vec-E1000-LR0.1-27    0.844444  0.398577\nNC-node2vec-E1000-LR0.01-28   0.844444  0.453816\nNC-node2vec-E1000-LR1e-05-29  0.569444  0.512059\nAssume Most Common Class      0.844444  0.392050\n\n[61 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NC-deepwalk-E5-LR5-0</th>\n      <td>0.780556</td>\n      <td>0.416198</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR1-1</th>\n      <td>0.844444</td>\n      <td>0.400739</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR0.1-2</th>\n      <td>0.550000</td>\n      <td>0.509405</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR0.01-3</th>\n      <td>0.468056</td>\n      <td>0.513663</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR1e-05-4</th>\n      <td>0.516667</td>\n      <td>0.513796</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR1-26</th>\n      <td>0.844444</td>\n      <td>0.398353</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR0.1-27</th>\n      <td>0.844444</td>\n      <td>0.398577</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR0.01-28</th>\n      <td>0.844444</td>\n      <td>0.453816</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR1e-05-29</th>\n      <td>0.569444</td>\n      <td>0.512059</td>\n    </tr>\n    <tr>\n      <th>Assume Most Common Class</th>\n      <td>0.844444</td>\n      <td>0.392050</td>\n    </tr>\n  </tbody>\n</table>\n<p>61 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame((nc_results.drop(columns=['Loss']).sum(1) / len(nc_results.columns)), columns=['Accuracy']).merge(nc_results['Loss'], left_index=True, right_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                              Does not calculate the number of digits  \\\nNC-deepwalk-E5-LR5-0                                         0.950000   \nNC-deepwalk-E5-LR1-1                                         0.950000   \nNC-deepwalk-E5-LR0.1-2                                       0.050000   \nNC-deepwalk-E5-LR0.01-3                                      0.050000   \nNC-deepwalk-E5-LR1e-05-4                                     0.950000   \nNC-deepwalk-E10-LR5-5                                        0.950000   \nNC-deepwalk-E10-LR1-6                                        0.950000   \nNC-deepwalk-E10-LR0.1-7                                      0.950000   \nNC-deepwalk-E10-LR0.01-8                                     0.050000   \nNC-deepwalk-E10-LR1e-05-9                                    0.950000   \nNC-deepwalk-E50-LR5-10                                       0.950000   \nNC-deepwalk-E50-LR1-11                                       0.950000   \nNC-deepwalk-E50-LR0.1-12                                     0.950000   \nNC-deepwalk-E50-LR0.01-13                                    0.050000   \nNC-deepwalk-E50-LR1e-05-14                                   0.050000   \nNC-deepwalk-E100-LR5-15                                      0.950000   \nNC-deepwalk-E100-LR1-16                                      0.950000   \nNC-deepwalk-E100-LR0.1-17                                    0.950000   \nNC-deepwalk-E100-LR0.01-18                                   0.950000   \nNC-deepwalk-E100-LR1e-05-19                                  0.033333   \nNC-deepwalk-E500-LR5-20                                      0.950000   \nNC-deepwalk-E500-LR1-21                                      0.950000   \nNC-deepwalk-E500-LR0.1-22                                    0.950000   \nNC-deepwalk-E500-LR0.01-23                                   0.950000   \nNC-deepwalk-E500-LR1e-05-24                                  0.050000   \nNC-deepwalk-E1000-LR5-25                                     0.950000   \nNC-deepwalk-E1000-LR1-26                                     0.950000   \nNC-deepwalk-E1000-LR0.1-27                                   0.950000   \nNC-deepwalk-E1000-LR0.01-28                                  0.950000   \nNC-deepwalk-E1000-LR1e-05-29                                 0.050000   \nNC-node2vec-E5-LR5-0                                         0.950000   \nNC-node2vec-E5-LR1-1                                         0.950000   \nNC-node2vec-E5-LR0.1-2                                       0.950000   \nNC-node2vec-E5-LR0.01-3                                      0.950000   \nNC-node2vec-E5-LR1e-05-4                                     0.050000   \nNC-node2vec-E10-LR5-5                                        0.950000   \nNC-node2vec-E10-LR1-6                                        0.950000   \nNC-node2vec-E10-LR0.1-7                                      0.950000   \nNC-node2vec-E10-LR0.01-8                                     0.050000   \nNC-node2vec-E10-LR1e-05-9                                    0.950000   \nNC-node2vec-E50-LR5-10                                       0.950000   \nNC-node2vec-E50-LR1-11                                       0.950000   \nNC-node2vec-E50-LR0.1-12                                     0.950000   \nNC-node2vec-E50-LR0.01-13                                    0.050000   \nNC-node2vec-E50-LR1e-05-14                                   0.950000   \nNC-node2vec-E100-LR5-15                                      0.950000   \nNC-node2vec-E100-LR1-16                                      0.950000   \nNC-node2vec-E100-LR0.1-17                                    0.950000   \nNC-node2vec-E100-LR0.01-18                                   0.950000   \nNC-node2vec-E100-LR1e-05-19                                  0.050000   \nNC-node2vec-E500-LR5-20                                      0.950000   \nNC-node2vec-E500-LR1-21                                      0.950000   \nNC-node2vec-E500-LR0.1-22                                    0.950000   \nNC-node2vec-E500-LR0.01-23                                   0.950000   \nNC-node2vec-E500-LR1e-05-24                                  0.950000   \nNC-node2vec-E1000-LR5-25                                     0.950000   \nNC-node2vec-E1000-LR1-26                                     0.950000   \nNC-node2vec-E1000-LR0.1-27                                   0.950000   \nNC-node2vec-E1000-LR0.01-28                                  0.950000   \nNC-node2vec-E1000-LR1e-05-29                                 0.950000   \n\n                              Reads positive numbers and continues reading  \\\nNC-deepwalk-E5-LR5-0                                              0.966667   \nNC-deepwalk-E5-LR1-1                                              0.966667   \nNC-deepwalk-E5-LR0.1-2                                            0.966667   \nNC-deepwalk-E5-LR0.01-3                                           0.033333   \nNC-deepwalk-E5-LR1e-05-4                                          0.033333   \nNC-deepwalk-E10-LR5-5                                             0.966667   \nNC-deepwalk-E10-LR1-6                                             0.966667   \nNC-deepwalk-E10-LR0.1-7                                           0.966667   \nNC-deepwalk-E10-LR0.01-8                                          0.033333   \nNC-deepwalk-E10-LR1e-05-9                                         0.033333   \nNC-deepwalk-E50-LR5-10                                            0.966667   \nNC-deepwalk-E50-LR1-11                                            0.966667   \nNC-deepwalk-E50-LR0.1-12                                          0.966667   \nNC-deepwalk-E50-LR0.01-13                                         0.033333   \nNC-deepwalk-E50-LR1e-05-14                                        0.966667   \nNC-deepwalk-E100-LR5-15                                           0.966667   \nNC-deepwalk-E100-LR1-16                                           0.966667   \nNC-deepwalk-E100-LR0.1-17                                         0.966667   \nNC-deepwalk-E100-LR0.01-18                                        0.083333   \nNC-deepwalk-E100-LR1e-05-19                                       0.033333   \nNC-deepwalk-E500-LR5-20                                           0.966667   \nNC-deepwalk-E500-LR1-21                                           0.966667   \nNC-deepwalk-E500-LR0.1-22                                         0.966667   \nNC-deepwalk-E500-LR0.01-23                                        0.966667   \nNC-deepwalk-E500-LR1e-05-24                                       0.033333   \nNC-deepwalk-E1000-LR5-25                                          0.983333   \nNC-deepwalk-E1000-LR1-26                                          0.950000   \nNC-deepwalk-E1000-LR0.1-27                                        0.966667   \nNC-deepwalk-E1000-LR0.01-28                                       0.966667   \nNC-deepwalk-E1000-LR1e-05-29                                      0.966667   \nNC-node2vec-E5-LR5-0                                              0.966667   \nNC-node2vec-E5-LR1-1                                              0.966667   \nNC-node2vec-E5-LR0.1-2                                            0.033333   \nNC-node2vec-E5-LR0.01-3                                           0.966667   \nNC-node2vec-E5-LR1e-05-4                                          0.033333   \nNC-node2vec-E10-LR5-5                                             0.966667   \nNC-node2vec-E10-LR1-6                                             0.966667   \nNC-node2vec-E10-LR0.1-7                                           0.966667   \nNC-node2vec-E10-LR0.01-8                                          0.966667   \nNC-node2vec-E10-LR1e-05-9                                         0.966667   \nNC-node2vec-E50-LR5-10                                            0.966667   \nNC-node2vec-E50-LR1-11                                            0.966667   \nNC-node2vec-E50-LR0.1-12                                          0.966667   \nNC-node2vec-E50-LR0.01-13                                         0.966667   \nNC-node2vec-E50-LR1e-05-14                                        0.966667   \nNC-node2vec-E100-LR5-15                                           0.966667   \nNC-node2vec-E100-LR1-16                                           0.966667   \nNC-node2vec-E100-LR0.1-17                                         0.966667   \nNC-node2vec-E100-LR0.01-18                                        0.966667   \nNC-node2vec-E100-LR1e-05-19                                       0.033333   \nNC-node2vec-E500-LR5-20                                           0.966667   \nNC-node2vec-E500-LR1-21                                           0.966667   \nNC-node2vec-E500-LR0.1-22                                         0.966667   \nNC-node2vec-E500-LR0.01-23                                        0.966667   \nNC-node2vec-E500-LR1e-05-24                                       0.966667   \nNC-node2vec-E1000-LR5-25                                          0.966667   \nNC-node2vec-E1000-LR1-26                                          0.966667   \nNC-node2vec-E1000-LR0.1-27                                        0.966667   \nNC-node2vec-E1000-LR0.01-28                                       0.966667   \nNC-node2vec-E1000-LR1e-05-29                                      0.966667   \n\n                              Can handle INT MAX and get the correct answer  \\\nNC-deepwalk-E5-LR5-0                                               0.950000   \nNC-deepwalk-E5-LR1-1                                               0.950000   \nNC-deepwalk-E5-LR0.1-2                                             0.950000   \nNC-deepwalk-E5-LR0.01-3                                            0.950000   \nNC-deepwalk-E5-LR1e-05-4                                           0.050000   \nNC-deepwalk-E10-LR5-5                                              0.950000   \nNC-deepwalk-E10-LR1-6                                              0.950000   \nNC-deepwalk-E10-LR0.1-7                                            0.850000   \nNC-deepwalk-E10-LR0.01-8                                           0.950000   \nNC-deepwalk-E10-LR1e-05-9                                          0.483333   \nNC-deepwalk-E50-LR5-10                                             0.950000   \nNC-deepwalk-E50-LR1-11                                             0.950000   \nNC-deepwalk-E50-LR0.1-12                                           0.950000   \nNC-deepwalk-E50-LR0.01-13                                          0.116667   \nNC-deepwalk-E50-LR1e-05-14                                         0.050000   \nNC-deepwalk-E100-LR5-15                                            0.950000   \nNC-deepwalk-E100-LR1-16                                            0.950000   \nNC-deepwalk-E100-LR0.1-17                                          0.950000   \nNC-deepwalk-E100-LR0.01-18                                         0.950000   \nNC-deepwalk-E100-LR1e-05-19                                        0.933333   \nNC-deepwalk-E500-LR5-20                                            0.950000   \nNC-deepwalk-E500-LR1-21                                            0.950000   \nNC-deepwalk-E500-LR0.1-22                                          0.950000   \nNC-deepwalk-E500-LR0.01-23                                         0.950000   \nNC-deepwalk-E500-LR1e-05-24                                        0.950000   \nNC-deepwalk-E1000-LR5-25                                           0.966667   \nNC-deepwalk-E1000-LR1-26                                           0.950000   \nNC-deepwalk-E1000-LR0.1-27                                         0.950000   \nNC-deepwalk-E1000-LR0.01-28                                        0.950000   \nNC-deepwalk-E1000-LR1e-05-29                                       0.083333   \nNC-node2vec-E5-LR5-0                                               0.950000   \nNC-node2vec-E5-LR1-1                                               0.950000   \nNC-node2vec-E5-LR0.1-2                                             0.950000   \nNC-node2vec-E5-LR0.01-3                                            0.950000   \nNC-node2vec-E5-LR1e-05-4                                           0.050000   \nNC-node2vec-E10-LR5-5                                              0.950000   \nNC-node2vec-E10-LR1-6                                              0.950000   \nNC-node2vec-E10-LR0.1-7                                            0.950000   \nNC-node2vec-E10-LR0.01-8                                           0.950000   \nNC-node2vec-E10-LR1e-05-9                                          0.950000   \nNC-node2vec-E50-LR5-10                                             0.950000   \nNC-node2vec-E50-LR1-11                                             0.950000   \nNC-node2vec-E50-LR0.1-12                                           0.950000   \nNC-node2vec-E50-LR0.01-13                                          0.050000   \nNC-node2vec-E50-LR1e-05-14                                         0.950000   \nNC-node2vec-E100-LR5-15                                            0.950000   \nNC-node2vec-E100-LR1-16                                            0.950000   \nNC-node2vec-E100-LR0.1-17                                          0.950000   \nNC-node2vec-E100-LR0.01-18                                         0.950000   \nNC-node2vec-E100-LR1e-05-19                                        0.950000   \nNC-node2vec-E500-LR5-20                                            0.950000   \nNC-node2vec-E500-LR1-21                                            0.950000   \nNC-node2vec-E500-LR0.1-22                                          0.950000   \nNC-node2vec-E500-LR0.01-23                                         0.950000   \nNC-node2vec-E500-LR1e-05-24                                        0.950000   \nNC-node2vec-E1000-LR5-25                                           0.950000   \nNC-node2vec-E1000-LR1-26                                           0.950000   \nNC-node2vec-E1000-LR0.1-27                                         0.950000   \nNC-node2vec-E1000-LR0.01-28                                        0.950000   \nNC-node2vec-E1000-LR1e-05-29                                       0.950000   \n\n                              Did not test negative numbers  \\\nNC-deepwalk-E5-LR5-0                               0.816667   \nNC-deepwalk-E5-LR1-1                               0.816667   \nNC-deepwalk-E5-LR0.1-2                             0.183333   \nNC-deepwalk-E5-LR0.01-3                            0.816667   \nNC-deepwalk-E5-LR1e-05-4                           0.183333   \nNC-deepwalk-E10-LR5-5                              0.816667   \nNC-deepwalk-E10-LR1-6                              0.816667   \nNC-deepwalk-E10-LR0.1-7                            0.466667   \nNC-deepwalk-E10-LR0.01-8                           0.183333   \nNC-deepwalk-E10-LR1e-05-9                          0.816667   \nNC-deepwalk-E50-LR5-10                             0.933333   \nNC-deepwalk-E50-LR1-11                             0.816667   \nNC-deepwalk-E50-LR0.1-12                           0.816667   \nNC-deepwalk-E50-LR0.01-13                          0.566667   \nNC-deepwalk-E50-LR1e-05-14                         0.816667   \nNC-deepwalk-E100-LR5-15                            0.883333   \nNC-deepwalk-E100-LR1-16                            0.816667   \nNC-deepwalk-E100-LR0.1-17                          0.816667   \nNC-deepwalk-E100-LR0.01-18                         0.816667   \nNC-deepwalk-E100-LR1e-05-19                        0.183333   \nNC-deepwalk-E500-LR5-20                            0.916667   \nNC-deepwalk-E500-LR1-21                            0.850000   \nNC-deepwalk-E500-LR0.1-22                          0.816667   \nNC-deepwalk-E500-LR0.01-23                         0.816667   \nNC-deepwalk-E500-LR1e-05-24                        0.816667   \nNC-deepwalk-E1000-LR5-25                           0.950000   \nNC-deepwalk-E1000-LR1-26                           0.733333   \nNC-deepwalk-E1000-LR0.1-27                         0.816667   \nNC-deepwalk-E1000-LR0.01-28                        0.816667   \nNC-deepwalk-E1000-LR1e-05-29                       0.233333   \nNC-node2vec-E5-LR5-0                               0.816667   \nNC-node2vec-E5-LR1-1                               0.816667   \nNC-node2vec-E5-LR0.1-2                             0.183333   \nNC-node2vec-E5-LR0.01-3                            0.183333   \nNC-node2vec-E5-LR1e-05-4                           0.816667   \nNC-node2vec-E10-LR5-5                              0.816667   \nNC-node2vec-E10-LR1-6                              0.816667   \nNC-node2vec-E10-LR0.1-7                            0.183333   \nNC-node2vec-E10-LR0.01-8                           0.183333   \nNC-node2vec-E10-LR1e-05-9                          0.816667   \nNC-node2vec-E50-LR5-10                             0.816667   \nNC-node2vec-E50-LR1-11                             0.816667   \nNC-node2vec-E50-LR0.1-12                           0.816667   \nNC-node2vec-E50-LR0.01-13                          0.816667   \nNC-node2vec-E50-LR1e-05-14                         0.183333   \nNC-node2vec-E100-LR5-15                            0.816667   \nNC-node2vec-E100-LR1-16                            0.816667   \nNC-node2vec-E100-LR0.1-17                          0.816667   \nNC-node2vec-E100-LR0.01-18                         0.816667   \nNC-node2vec-E100-LR1e-05-19                        0.183333   \nNC-node2vec-E500-LR5-20                            0.816667   \nNC-node2vec-E500-LR1-21                            0.816667   \nNC-node2vec-E500-LR0.1-22                          0.816667   \nNC-node2vec-E500-LR0.01-23                         0.816667   \nNC-node2vec-E500-LR1e-05-24                        0.816667   \nNC-node2vec-E1000-LR5-25                           0.816667   \nNC-node2vec-E1000-LR1-26                           0.816667   \nNC-node2vec-E1000-LR0.1-27                         0.816667   \nNC-node2vec-E1000-LR0.01-28                        0.816667   \nNC-node2vec-E1000-LR1e-05-29                       0.816667   \n\n                              Output matches correct implementation  Compiles  \\\nNC-deepwalk-E5-LR5-0                                       0.900000  1.000000   \nNC-deepwalk-E5-LR1-1                                       0.900000  1.000000   \nNC-deepwalk-E5-LR0.1-2                                     0.900000  0.866667   \nNC-deepwalk-E5-LR0.01-3                                    0.900000  0.000000   \nNC-deepwalk-E5-LR1e-05-4                                   0.900000  1.000000   \nNC-deepwalk-E10-LR5-5                                      0.900000  1.000000   \nNC-deepwalk-E10-LR1-6                                      0.900000  1.000000   \nNC-deepwalk-E10-LR0.1-7                                    0.766667  0.950000   \nNC-deepwalk-E10-LR0.01-8                                   0.116667  1.000000   \nNC-deepwalk-E10-LR1e-05-9                                  0.100000  0.000000   \nNC-deepwalk-E50-LR5-10                                     0.933333  1.000000   \nNC-deepwalk-E50-LR1-11                                     0.900000  1.000000   \nNC-deepwalk-E50-LR0.1-12                                   0.900000  1.000000   \nNC-deepwalk-E50-LR0.01-13                                  0.900000  0.000000   \nNC-deepwalk-E50-LR1e-05-14                                 0.900000  0.666667   \nNC-deepwalk-E100-LR5-15                                    0.933333  1.000000   \nNC-deepwalk-E100-LR1-16                                    0.900000  1.000000   \nNC-deepwalk-E100-LR0.1-17                                  0.900000  1.000000   \nNC-deepwalk-E100-LR0.01-18                                 0.650000  1.000000   \nNC-deepwalk-E100-LR1e-05-19                                0.900000  1.000000   \nNC-deepwalk-E500-LR5-20                                    0.933333  1.000000   \nNC-deepwalk-E500-LR1-21                                    0.916667  1.000000   \nNC-deepwalk-E500-LR0.1-22                                  0.900000  1.000000   \nNC-deepwalk-E500-LR0.01-23                                 0.900000  1.000000   \nNC-deepwalk-E500-LR1e-05-24                                0.900000  1.000000   \nNC-deepwalk-E1000-LR5-25                                   0.950000  1.000000   \nNC-deepwalk-E1000-LR1-26                                   0.933333  1.000000   \nNC-deepwalk-E1000-LR0.1-27                                 0.900000  1.000000   \nNC-deepwalk-E1000-LR0.01-28                                0.900000  1.000000   \nNC-deepwalk-E1000-LR1e-05-29                               0.750000  1.000000   \nNC-node2vec-E5-LR5-0                                       0.900000  1.000000   \nNC-node2vec-E5-LR1-1                                       0.900000  1.000000   \nNC-node2vec-E5-LR0.1-2                                     0.900000  1.000000   \nNC-node2vec-E5-LR0.01-3                                    0.100000  1.000000   \nNC-node2vec-E5-LR1e-05-4                                   0.900000  1.000000   \nNC-node2vec-E10-LR5-5                                      0.900000  1.000000   \nNC-node2vec-E10-LR1-6                                      0.900000  1.000000   \nNC-node2vec-E10-LR0.1-7                                    0.900000  1.000000   \nNC-node2vec-E10-LR0.01-8                                   0.900000  0.000000   \nNC-node2vec-E10-LR1e-05-9                                  0.900000  1.000000   \nNC-node2vec-E50-LR5-10                                     0.900000  1.000000   \nNC-node2vec-E50-LR1-11                                     0.900000  1.000000   \nNC-node2vec-E50-LR0.1-12                                   0.900000  1.000000   \nNC-node2vec-E50-LR0.01-13                                  0.100000  0.000000   \nNC-node2vec-E50-LR1e-05-14                                 0.900000  1.000000   \nNC-node2vec-E100-LR5-15                                    0.900000  1.000000   \nNC-node2vec-E100-LR1-16                                    0.900000  1.000000   \nNC-node2vec-E100-LR0.1-17                                  0.900000  1.000000   \nNC-node2vec-E100-LR0.01-18                                 0.100000  1.000000   \nNC-node2vec-E100-LR1e-05-19                                0.100000  1.000000   \nNC-node2vec-E500-LR5-20                                    0.900000  1.000000   \nNC-node2vec-E500-LR1-21                                    0.900000  1.000000   \nNC-node2vec-E500-LR0.1-22                                  0.900000  1.000000   \nNC-node2vec-E500-LR0.01-23                                 0.900000  1.000000   \nNC-node2vec-E500-LR1e-05-24                                0.900000  1.000000   \nNC-node2vec-E1000-LR5-25                                   0.900000  1.000000   \nNC-node2vec-E1000-LR1-26                                   0.900000  1.000000   \nNC-node2vec-E1000-LR0.1-27                                 0.900000  1.000000   \nNC-node2vec-E1000-LR0.01-28                                0.900000  1.000000   \nNC-node2vec-E1000-LR1e-05-29                               0.100000  0.000000   \n\n                              Reads positive and negative numbers and continues reading until -1  \\\nNC-deepwalk-E5-LR5-0                                                   0.783333                    \nNC-deepwalk-E5-LR1-1                                                   0.783333                    \nNC-deepwalk-E5-LR0.1-2                                                 0.783333                    \nNC-deepwalk-E5-LR0.01-3                                                0.783333                    \nNC-deepwalk-E5-LR1e-05-4                                               0.216667                    \nNC-deepwalk-E10-LR5-5                                                  0.783333                    \nNC-deepwalk-E10-LR1-6                                                  0.783333                    \nNC-deepwalk-E10-LR0.1-7                                                0.783333                    \nNC-deepwalk-E10-LR0.01-8                                               0.216667                    \nNC-deepwalk-E10-LR1e-05-9                                              0.783333                    \nNC-deepwalk-E50-LR5-10                                                 0.916667                    \nNC-deepwalk-E50-LR1-11                                                 0.783333                    \nNC-deepwalk-E50-LR0.1-12                                               0.783333                    \nNC-deepwalk-E50-LR0.01-13                                              0.216667                    \nNC-deepwalk-E50-LR1e-05-14                                             0.783333                    \nNC-deepwalk-E100-LR5-15                                                0.866667                    \nNC-deepwalk-E100-LR1-16                                                0.766667                    \nNC-deepwalk-E100-LR0.1-17                                              0.783333                    \nNC-deepwalk-E100-LR0.01-18                                             0.216667                    \nNC-deepwalk-E100-LR1e-05-19                                            0.216667                    \nNC-deepwalk-E500-LR5-20                                                0.966667                    \nNC-deepwalk-E500-LR1-21                                                0.883333                    \nNC-deepwalk-E500-LR0.1-22                                              0.783333                    \nNC-deepwalk-E500-LR0.01-23                                             0.783333                    \nNC-deepwalk-E500-LR1e-05-24                                            0.283333                    \nNC-deepwalk-E1000-LR5-25                                               0.950000                    \nNC-deepwalk-E1000-LR1-26                                               0.883333                    \nNC-deepwalk-E1000-LR0.1-27                                             0.800000                    \nNC-deepwalk-E1000-LR0.01-28                                            0.783333                    \nNC-deepwalk-E1000-LR1e-05-29                                           0.216667                    \nNC-node2vec-E5-LR5-0                                                   0.783333                    \nNC-node2vec-E5-LR1-1                                                   0.216667                    \nNC-node2vec-E5-LR0.1-2                                                 0.783333                    \nNC-node2vec-E5-LR0.01-3                                                0.783333                    \nNC-node2vec-E5-LR1e-05-4                                               0.216667                    \nNC-node2vec-E10-LR5-5                                                  0.783333                    \nNC-node2vec-E10-LR1-6                                                  0.783333                    \nNC-node2vec-E10-LR0.1-7                                                0.783333                    \nNC-node2vec-E10-LR0.01-8                                               0.216667                    \nNC-node2vec-E10-LR1e-05-9                                              0.216667                    \nNC-node2vec-E50-LR5-10                                                 0.783333                    \nNC-node2vec-E50-LR1-11                                                 0.783333                    \nNC-node2vec-E50-LR0.1-12                                               0.783333                    \nNC-node2vec-E50-LR0.01-13                                              0.783333                    \nNC-node2vec-E50-LR1e-05-14                                             0.783333                    \nNC-node2vec-E100-LR5-15                                                0.783333                    \nNC-node2vec-E100-LR1-16                                                0.783333                    \nNC-node2vec-E100-LR0.1-17                                              0.783333                    \nNC-node2vec-E100-LR0.01-18                                             0.783333                    \nNC-node2vec-E100-LR1e-05-19                                            0.783333                    \nNC-node2vec-E500-LR5-20                                                0.783333                    \nNC-node2vec-E500-LR1-21                                                0.783333                    \nNC-node2vec-E500-LR0.1-22                                              0.783333                    \nNC-node2vec-E500-LR0.01-23                                             0.783333                    \nNC-node2vec-E500-LR1e-05-24                                            0.783333                    \nNC-node2vec-E1000-LR5-25                                               0.783333                    \nNC-node2vec-E1000-LR1-26                                               0.783333                    \nNC-node2vec-E1000-LR0.1-27                                             0.783333                    \nNC-node2vec-E1000-LR0.01-28                                            0.783333                    \nNC-node2vec-E1000-LR1e-05-29                                           0.216667                    \n\n                              Does not double 0 into 00  \\\nNC-deepwalk-E5-LR5-0                           0.950000   \nNC-deepwalk-E5-LR1-1                           0.950000   \nNC-deepwalk-E5-LR0.1-2                         0.950000   \nNC-deepwalk-E5-LR0.01-3                        0.850000   \nNC-deepwalk-E5-LR1e-05-4                       0.050000   \nNC-deepwalk-E10-LR5-5                          0.950000   \nNC-deepwalk-E10-LR1-6                          0.950000   \nNC-deepwalk-E10-LR0.1-7                        0.950000   \nNC-deepwalk-E10-LR0.01-8                       0.950000   \nNC-deepwalk-E10-LR1e-05-9                      0.050000   \nNC-deepwalk-E50-LR5-10                         0.983333   \nNC-deepwalk-E50-LR1-11                         0.950000   \nNC-deepwalk-E50-LR0.1-12                       0.950000   \nNC-deepwalk-E50-LR0.01-13                      0.050000   \nNC-deepwalk-E50-LR1e-05-14                     0.866667   \nNC-deepwalk-E100-LR5-15                        0.983333   \nNC-deepwalk-E100-LR1-16                        0.950000   \nNC-deepwalk-E100-LR0.1-17                      0.950000   \nNC-deepwalk-E100-LR0.01-18                     0.400000   \nNC-deepwalk-E100-LR1e-05-19                    0.050000   \nNC-deepwalk-E500-LR5-20                        0.983333   \nNC-deepwalk-E500-LR1-21                        0.950000   \nNC-deepwalk-E500-LR0.1-22                      0.950000   \nNC-deepwalk-E500-LR0.01-23                     0.950000   \nNC-deepwalk-E500-LR1e-05-24                    0.950000   \nNC-deepwalk-E1000-LR5-25                       1.000000   \nNC-deepwalk-E1000-LR1-26                       0.983333   \nNC-deepwalk-E1000-LR0.1-27                     0.950000   \nNC-deepwalk-E1000-LR0.01-28                    0.950000   \nNC-deepwalk-E1000-LR1e-05-29                   0.050000   \nNC-node2vec-E5-LR5-0                           0.950000   \nNC-node2vec-E5-LR1-1                           0.950000   \nNC-node2vec-E5-LR0.1-2                         0.050000   \nNC-node2vec-E5-LR0.01-3                        0.050000   \nNC-node2vec-E5-LR1e-05-4                       0.950000   \nNC-node2vec-E10-LR5-5                          0.950000   \nNC-node2vec-E10-LR1-6                          0.950000   \nNC-node2vec-E10-LR0.1-7                        0.950000   \nNC-node2vec-E10-LR0.01-8                       0.950000   \nNC-node2vec-E10-LR1e-05-9                      0.050000   \nNC-node2vec-E50-LR5-10                         0.950000   \nNC-node2vec-E50-LR1-11                         0.950000   \nNC-node2vec-E50-LR0.1-12                       0.950000   \nNC-node2vec-E50-LR0.01-13                      0.950000   \nNC-node2vec-E50-LR1e-05-14                     0.050000   \nNC-node2vec-E100-LR5-15                        0.950000   \nNC-node2vec-E100-LR1-16                        0.950000   \nNC-node2vec-E100-LR0.1-17                      0.950000   \nNC-node2vec-E100-LR0.01-18                     0.050000   \nNC-node2vec-E100-LR1e-05-19                    0.050000   \nNC-node2vec-E500-LR5-20                        0.950000   \nNC-node2vec-E500-LR1-21                        0.950000   \nNC-node2vec-E500-LR0.1-22                      0.950000   \nNC-node2vec-E500-LR0.01-23                     0.950000   \nNC-node2vec-E500-LR1e-05-24                    0.950000   \nNC-node2vec-E1000-LR5-25                       0.950000   \nNC-node2vec-E1000-LR1-26                       0.950000   \nNC-node2vec-E1000-LR0.1-27                     0.950000   \nNC-node2vec-E1000-LR0.01-28                    0.950000   \nNC-node2vec-E1000-LR1e-05-29                   0.950000   \n\n                              Does not use System.exit to leave the program  \\\nNC-deepwalk-E5-LR5-0                                               0.116667   \nNC-deepwalk-E5-LR1-1                                               0.883333   \nNC-deepwalk-E5-LR0.1-2                                             0.883333   \nNC-deepwalk-E5-LR0.01-3                                            0.883333   \nNC-deepwalk-E5-LR1e-05-4                                           0.883333   \nNC-deepwalk-E10-LR5-5                                              0.883333   \nNC-deepwalk-E10-LR1-6                                              0.883333   \nNC-deepwalk-E10-LR0.1-7                                            0.883333   \nNC-deepwalk-E10-LR0.01-8                                           0.883333   \nNC-deepwalk-E10-LR1e-05-9                                          0.883333   \nNC-deepwalk-E50-LR5-10                                             0.833333   \nNC-deepwalk-E50-LR1-11                                             0.416667   \nNC-deepwalk-E50-LR0.1-12                                           0.883333   \nNC-deepwalk-E50-LR0.01-13                                          0.883333   \nNC-deepwalk-E50-LR1e-05-14                                         0.116667   \nNC-deepwalk-E100-LR5-15                                            0.900000   \nNC-deepwalk-E100-LR1-16                                            0.600000   \nNC-deepwalk-E100-LR0.1-17                                          0.883333   \nNC-deepwalk-E100-LR0.01-18                                         0.116667   \nNC-deepwalk-E100-LR1e-05-19                                        0.116667   \nNC-deepwalk-E500-LR5-20                                            0.900000   \nNC-deepwalk-E500-LR1-21                                            0.816667   \nNC-deepwalk-E500-LR0.1-22                                          0.966667   \nNC-deepwalk-E500-LR0.01-23                                         0.883333   \nNC-deepwalk-E500-LR1e-05-24                                        0.833333   \nNC-deepwalk-E1000-LR5-25                                           0.966667   \nNC-deepwalk-E1000-LR1-26                                           0.683333   \nNC-deepwalk-E1000-LR0.1-27                                         0.700000   \nNC-deepwalk-E1000-LR0.01-28                                        0.883333   \nNC-deepwalk-E1000-LR1e-05-29                                       0.116667   \nNC-node2vec-E5-LR5-0                                               0.883333   \nNC-node2vec-E5-LR1-1                                               0.883333   \nNC-node2vec-E5-LR0.1-2                                             0.116667   \nNC-node2vec-E5-LR0.01-3                                            0.883333   \nNC-node2vec-E5-LR1e-05-4                                           0.116667   \nNC-node2vec-E10-LR5-5                                              0.883333   \nNC-node2vec-E10-LR1-6                                              0.116667   \nNC-node2vec-E10-LR0.1-7                                            0.883333   \nNC-node2vec-E10-LR0.01-8                                           0.116667   \nNC-node2vec-E10-LR1e-05-9                                          0.883333   \nNC-node2vec-E50-LR5-10                                             0.883333   \nNC-node2vec-E50-LR1-11                                             0.883333   \nNC-node2vec-E50-LR0.1-12                                           0.883333   \nNC-node2vec-E50-LR0.01-13                                          0.883333   \nNC-node2vec-E50-LR1e-05-14                                         0.883333   \nNC-node2vec-E100-LR5-15                                            0.883333   \nNC-node2vec-E100-LR1-16                                            0.883333   \nNC-node2vec-E100-LR0.1-17                                          0.883333   \nNC-node2vec-E100-LR0.01-18                                         0.883333   \nNC-node2vec-E100-LR1e-05-19                                        0.883333   \nNC-node2vec-E500-LR5-20                                            0.883333   \nNC-node2vec-E500-LR1-21                                            0.883333   \nNC-node2vec-E500-LR0.1-22                                          0.883333   \nNC-node2vec-E500-LR0.01-23                                         0.883333   \nNC-node2vec-E500-LR1e-05-24                                        0.883333   \nNC-node2vec-E1000-LR5-25                                           0.883333   \nNC-node2vec-E1000-LR1-26                                           0.883333   \nNC-node2vec-E1000-LR0.1-27                                         0.883333   \nNC-node2vec-E1000-LR0.01-28                                        0.883333   \nNC-node2vec-E1000-LR1e-05-29                                       0.883333   \n\n                              Can handle INT MAX  Scanner used correctly  \\\nNC-deepwalk-E5-LR5-0                    0.966667                0.966667   \nNC-deepwalk-E5-LR1-1                    0.966667                0.966667   \nNC-deepwalk-E5-LR0.1-2                  0.033333                0.033333   \nNC-deepwalk-E5-LR0.01-3                 0.316667                0.033333   \nNC-deepwalk-E5-LR1e-05-4                0.966667                0.966667   \nNC-deepwalk-E10-LR5-5                   0.966667                0.966667   \nNC-deepwalk-E10-LR1-6                   0.966667                0.966667   \nNC-deepwalk-E10-LR0.1-7                 0.033333                0.966667   \nNC-deepwalk-E10-LR0.01-8                0.033333                0.933333   \nNC-deepwalk-E10-LR1e-05-9               0.966667                0.033333   \nNC-deepwalk-E50-LR5-10                  0.966667                0.966667   \nNC-deepwalk-E50-LR1-11                  0.966667                0.966667   \nNC-deepwalk-E50-LR0.1-12                0.966667                0.966667   \nNC-deepwalk-E50-LR0.01-13               0.966667                0.100000   \nNC-deepwalk-E50-LR1e-05-14              0.033333                0.033333   \nNC-deepwalk-E100-LR5-15                 0.966667                0.966667   \nNC-deepwalk-E100-LR1-16                 0.966667                0.966667   \nNC-deepwalk-E100-LR0.1-17               0.966667                0.966667   \nNC-deepwalk-E100-LR0.01-18              0.566667                0.716667   \nNC-deepwalk-E100-LR1e-05-19             0.033333                0.966667   \nNC-deepwalk-E500-LR5-20                 0.966667                0.966667   \nNC-deepwalk-E500-LR1-21                 0.966667                0.966667   \nNC-deepwalk-E500-LR0.1-22               0.966667                0.966667   \nNC-deepwalk-E500-LR0.01-23              0.966667                0.966667   \nNC-deepwalk-E500-LR1e-05-24             0.033333                0.966667   \nNC-deepwalk-E1000-LR5-25                0.983333                0.966667   \nNC-deepwalk-E1000-LR1-26                0.966667                0.966667   \nNC-deepwalk-E1000-LR0.1-27              0.966667                0.966667   \nNC-deepwalk-E1000-LR0.01-28             0.966667                0.966667   \nNC-deepwalk-E1000-LR1e-05-29            0.966667                0.966667   \nNC-node2vec-E5-LR5-0                    0.966667                0.966667   \nNC-node2vec-E5-LR1-1                    0.966667                0.966667   \nNC-node2vec-E5-LR0.1-2                  0.966667                0.966667   \nNC-node2vec-E5-LR0.01-3                 0.966667                0.033333   \nNC-node2vec-E5-LR1e-05-4                0.966667                0.966667   \nNC-node2vec-E10-LR5-5                   0.966667                0.966667   \nNC-node2vec-E10-LR1-6                   0.966667                0.966667   \nNC-node2vec-E10-LR0.1-7                 0.033333                0.966667   \nNC-node2vec-E10-LR0.01-8                0.033333                0.033333   \nNC-node2vec-E10-LR1e-05-9               0.033333                0.966667   \nNC-node2vec-E50-LR5-10                  0.966667                0.966667   \nNC-node2vec-E50-LR1-11                  0.966667                0.966667   \nNC-node2vec-E50-LR0.1-12                0.966667                0.966667   \nNC-node2vec-E50-LR0.01-13               0.033333                0.033333   \nNC-node2vec-E50-LR1e-05-14              0.966667                0.033333   \nNC-node2vec-E100-LR5-15                 0.966667                0.966667   \nNC-node2vec-E100-LR1-16                 0.966667                0.966667   \nNC-node2vec-E100-LR0.1-17               0.966667                0.966667   \nNC-node2vec-E100-LR0.01-18              0.033333                0.966667   \nNC-node2vec-E100-LR1e-05-19             0.033333                0.033333   \nNC-node2vec-E500-LR5-20                 0.966667                0.966667   \nNC-node2vec-E500-LR1-21                 0.966667                0.966667   \nNC-node2vec-E500-LR0.1-22               0.966667                0.966667   \nNC-node2vec-E500-LR0.01-23              0.966667                0.966667   \nNC-node2vec-E500-LR1e-05-24             0.966667                0.033333   \nNC-node2vec-E1000-LR5-25                0.966667                0.966667   \nNC-node2vec-E1000-LR1-26                0.966667                0.966667   \nNC-node2vec-E1000-LR0.1-27              0.966667                0.966667   \nNC-node2vec-E1000-LR0.01-28             0.966667                0.966667   \nNC-node2vec-E1000-LR1e-05-29            0.033333                0.966667   \n\n                                  Loss  \nNC-deepwalk-E5-LR5-0          0.416198  \nNC-deepwalk-E5-LR1-1          0.400739  \nNC-deepwalk-E5-LR0.1-2        0.509405  \nNC-deepwalk-E5-LR0.01-3       0.513663  \nNC-deepwalk-E5-LR1e-05-4      0.513796  \nNC-deepwalk-E10-LR5-5         0.395030  \nNC-deepwalk-E10-LR1-6         0.400548  \nNC-deepwalk-E10-LR0.1-7       0.505446  \nNC-deepwalk-E10-LR0.01-8      0.512004  \nNC-deepwalk-E10-LR1e-05-9     0.515393  \nNC-deepwalk-E50-LR5-10        0.376674  \nNC-deepwalk-E50-LR1-11        0.399556  \nNC-deepwalk-E50-LR0.1-12      0.399073  \nNC-deepwalk-E50-LR0.01-13     0.515957  \nNC-deepwalk-E50-LR1e-05-14    0.515631  \nNC-deepwalk-E100-LR5-15       0.373120  \nNC-deepwalk-E100-LR1-16       0.394752  \nNC-deepwalk-E100-LR0.1-17     0.399144  \nNC-deepwalk-E100-LR0.01-18    0.508963  \nNC-deepwalk-E100-LR1e-05-19   0.515011  \nNC-deepwalk-E500-LR5-20       0.368489  \nNC-deepwalk-E500-LR1-21       0.380282  \nNC-deepwalk-E500-LR0.1-22     0.393171  \nNC-deepwalk-E500-LR0.01-23    0.398579  \nNC-deepwalk-E500-LR1e-05-24   0.510679  \nNC-deepwalk-E1000-LR5-25      0.360979  \nNC-deepwalk-E1000-LR1-26      0.387189  \nNC-deepwalk-E1000-LR0.1-27    0.395503  \nNC-deepwalk-E1000-LR0.01-28   0.398642  \nNC-deepwalk-E1000-LR1e-05-29  0.511990  \nNC-node2vec-E5-LR5-0          0.409195  \nNC-node2vec-E5-LR1-1          0.481615  \nNC-node2vec-E5-LR0.1-2        0.510151  \nNC-node2vec-E5-LR0.01-3       0.511159  \nNC-node2vec-E5-LR1e-05-4      0.510064  \nNC-node2vec-E10-LR5-5         0.400801  \nNC-node2vec-E10-LR1-6         0.463111  \nNC-node2vec-E10-LR0.1-7       0.505894  \nNC-node2vec-E10-LR0.01-8      0.513352  \nNC-node2vec-E10-LR1e-05-9     0.509264  \nNC-node2vec-E50-LR5-10        0.399143  \nNC-node2vec-E50-LR1-11        0.399977  \nNC-node2vec-E50-LR0.1-12      0.485121  \nNC-node2vec-E50-LR0.01-13     0.513647  \nNC-node2vec-E50-LR1e-05-14    0.504780  \nNC-node2vec-E100-LR5-15       0.399114  \nNC-node2vec-E100-LR1-16       0.398806  \nNC-node2vec-E100-LR0.1-17     0.464643  \nNC-node2vec-E100-LR0.01-18    0.508617  \nNC-node2vec-E100-LR1e-05-19   0.514661  \nNC-node2vec-E500-LR5-20       0.398971  \nNC-node2vec-E500-LR1-21       0.398372  \nNC-node2vec-E500-LR0.1-22     0.399970  \nNC-node2vec-E500-LR0.01-23    0.479023  \nNC-node2vec-E500-LR1e-05-24   0.505603  \nNC-node2vec-E1000-LR5-25      0.398809  \nNC-node2vec-E1000-LR1-26      0.398353  \nNC-node2vec-E1000-LR0.1-27    0.398577  \nNC-node2vec-E1000-LR0.01-28   0.453816  \nNC-node2vec-E1000-LR1e-05-29  0.512059  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Does not calculate the number of digits</th>\n      <th>Reads positive numbers and continues reading</th>\n      <th>Can handle INT MAX and get the correct answer</th>\n      <th>Did not test negative numbers</th>\n      <th>Output matches correct implementation</th>\n      <th>Compiles</th>\n      <th>Reads positive and negative numbers and continues reading until -1</th>\n      <th>Does not double 0 into 00</th>\n      <th>Does not use System.exit to leave the program</th>\n      <th>Can handle INT MAX</th>\n      <th>Scanner used correctly</th>\n      <th>Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NC-deepwalk-E5-LR5-0</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.116667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.416198</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR1-1</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.400739</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR0.1-2</th>\n      <td>0.050000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>0.866667</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.509405</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR0.01-3</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>0.000000</td>\n      <td>0.783333</td>\n      <td>0.850000</td>\n      <td>0.883333</td>\n      <td>0.316667</td>\n      <td>0.033333</td>\n      <td>0.513663</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E5-LR1e-05-4</th>\n      <td>0.950000</td>\n      <td>0.033333</td>\n      <td>0.050000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.513796</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E10-LR5-5</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.395030</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E10-LR1-6</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.400548</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E10-LR0.1-7</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.850000</td>\n      <td>0.466667</td>\n      <td>0.766667</td>\n      <td>0.950000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.505446</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E10-LR0.01-8</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.116667</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.933333</td>\n      <td>0.512004</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E10-LR1e-05-9</th>\n      <td>0.950000</td>\n      <td>0.033333</td>\n      <td>0.483333</td>\n      <td>0.816667</td>\n      <td>0.100000</td>\n      <td>0.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.033333</td>\n      <td>0.515393</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E50-LR5-10</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.933333</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.916667</td>\n      <td>0.983333</td>\n      <td>0.833333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.376674</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E50-LR1-11</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.416667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399556</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E50-LR0.1-12</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399073</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E50-LR0.01-13</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.116667</td>\n      <td>0.566667</td>\n      <td>0.900000</td>\n      <td>0.000000</td>\n      <td>0.216667</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.100000</td>\n      <td>0.515957</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E50-LR1e-05-14</th>\n      <td>0.050000</td>\n      <td>0.966667</td>\n      <td>0.050000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>0.666667</td>\n      <td>0.783333</td>\n      <td>0.866667</td>\n      <td>0.116667</td>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.515631</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E100-LR5-15</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.866667</td>\n      <td>0.983333</td>\n      <td>0.900000</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.373120</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E100-LR1-16</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.766667</td>\n      <td>0.950000</td>\n      <td>0.600000</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.394752</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E100-LR0.1-17</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399144</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E100-LR0.01-18</th>\n      <td>0.950000</td>\n      <td>0.083333</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.650000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.400000</td>\n      <td>0.116667</td>\n      <td>0.566667</td>\n      <td>0.716667</td>\n      <td>0.508963</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E100-LR1e-05-19</th>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.933333</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.050000</td>\n      <td>0.116667</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.515011</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E500-LR5-20</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.916667</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.966667</td>\n      <td>0.983333</td>\n      <td>0.900000</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.368489</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E500-LR1-21</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.850000</td>\n      <td>0.916667</td>\n      <td>1.000000</td>\n      <td>0.883333</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.380282</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E500-LR0.1-22</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.393171</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E500-LR0.01-23</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398579</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E500-LR1e-05-24</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.283333</td>\n      <td>0.950000</td>\n      <td>0.833333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.510679</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E1000-LR5-25</th>\n      <td>0.950000</td>\n      <td>0.983333</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.950000</td>\n      <td>1.000000</td>\n      <td>0.950000</td>\n      <td>1.000000</td>\n      <td>0.966667</td>\n      <td>0.983333</td>\n      <td>0.966667</td>\n      <td>0.360979</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E1000-LR1-26</th>\n      <td>0.950000</td>\n      <td>0.950000</td>\n      <td>0.950000</td>\n      <td>0.733333</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.883333</td>\n      <td>0.983333</td>\n      <td>0.683333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.387189</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E1000-LR0.1-27</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>0.950000</td>\n      <td>0.700000</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.395503</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E1000-LR0.01-28</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398642</td>\n    </tr>\n    <tr>\n      <th>NC-deepwalk-E1000-LR1e-05-29</th>\n      <td>0.050000</td>\n      <td>0.966667</td>\n      <td>0.083333</td>\n      <td>0.233333</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.050000</td>\n      <td>0.116667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.511990</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E5-LR5-0</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.409195</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E5-LR1-1</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.481615</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E5-LR0.1-2</th>\n      <td>0.950000</td>\n      <td>0.033333</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.116667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.510151</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E5-LR0.01-3</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.033333</td>\n      <td>0.511159</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E5-LR1e-05-4</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.050000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.950000</td>\n      <td>0.116667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.510064</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E10-LR5-5</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.400801</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E10-LR1-6</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.116667</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.463111</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E10-LR0.1-7</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.505894</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E10-LR0.01-8</th>\n      <td>0.050000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>0.000000</td>\n      <td>0.216667</td>\n      <td>0.950000</td>\n      <td>0.116667</td>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.513352</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E10-LR1e-05-9</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.216667</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.509264</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E50-LR5-10</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399143</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E50-LR1-11</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399977</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E50-LR0.1-12</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.485121</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E50-LR0.01-13</th>\n      <td>0.050000</td>\n      <td>0.966667</td>\n      <td>0.050000</td>\n      <td>0.816667</td>\n      <td>0.100000</td>\n      <td>0.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.513647</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E50-LR1e-05-14</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.033333</td>\n      <td>0.504780</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E100-LR5-15</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399114</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E100-LR1-16</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398806</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E100-LR0.1-17</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.464643</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E100-LR0.01-18</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.508617</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E100-LR1e-05-19</th>\n      <td>0.050000</td>\n      <td>0.033333</td>\n      <td>0.950000</td>\n      <td>0.183333</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.050000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.033333</td>\n      <td>0.514661</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E500-LR5-20</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398971</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E500-LR1-21</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398372</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E500-LR0.1-22</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.399970</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E500-LR0.01-23</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.479023</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E500-LR1e-05-24</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.033333</td>\n      <td>0.505603</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR5-25</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398809</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR1-26</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398353</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR0.1-27</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.398577</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR0.01-28</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.900000</td>\n      <td>1.000000</td>\n      <td>0.783333</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.966667</td>\n      <td>0.966667</td>\n      <td>0.453816</td>\n    </tr>\n    <tr>\n      <th>NC-node2vec-E1000-LR1e-05-29</th>\n      <td>0.950000</td>\n      <td>0.966667</td>\n      <td>0.950000</td>\n      <td>0.816667</td>\n      <td>0.100000</td>\n      <td>0.000000</td>\n      <td>0.216667</td>\n      <td>0.950000</td>\n      <td>0.883333</td>\n      <td>0.033333</td>\n      <td>0.966667</td>\n      <td>0.512059</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                                           0\nDoes not calculate the number of digits             0.950000\nReads positive numbers and continues reading        0.966667\nCan handle INT MAX and get the correct answer       0.950000\nDid not test negative numbers                       0.816667\nOutput matches correct implementation               0.900000\nCompiles                                            1.000000\nReads positive and negative numbers and continu...  0.783333\nDoes not double 0 into 00                           0.950000\nDoes not use System.exit to leave the program       0.883333\nCan handle INT MAX                                  0.966667\nScanner used correctly                              0.966667\nLoss                                                0.392050",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Does not calculate the number of digits</th>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <th>Reads positive numbers and continues reading</th>\n      <td>0.966667</td>\n    </tr>\n    <tr>\n      <th>Can handle INT MAX and get the correct answer</th>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <th>Did not test negative numbers</th>\n      <td>0.816667</td>\n    </tr>\n    <tr>\n      <th>Output matches correct implementation</th>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>Compiles</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>Reads positive and negative numbers and continues reading until -1</th>\n      <td>0.783333</td>\n    </tr>\n    <tr>\n      <th>Does not double 0 into 00</th>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <th>Does not use System.exit to leave the program</th>\n      <td>0.883333</td>\n    </tr>\n    <tr>\n      <th>Can handle INT MAX</th>\n      <td>0.966667</td>\n    </tr>\n    <tr>\n      <th>Scanner used correctly</th>\n      <td>0.966667</td>\n    </tr>\n    <tr>\n      <th>Loss</th>\n      <td>0.392050</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(nc_results)\n",
    "mcc_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c2b_misconceptions = [\n",
    "    'Calculates the number of digits in the binary representation',\n",
    "    'Converts the number to a decimal representation of the binary number',\n",
    "    'Multiplies up to the number instead of using division',\n",
    "    'Conflates do-while and while loops',\n",
    "    'Did not handle negative numbers',\n",
    "]\n",
    "\n",
    "main_misconceptions = [\n",
    "    'Does not continue reading when given a positive number',\n",
    "    'Does not use proper control flow to finish the program',\n",
    "    'Uses Scanner.next() instead of Scanner.nextInt()',\n",
    "    'Does not quit when given -1',\n",
    "    'Does not reprompt user when given any other negative number',\n",
    "]\n",
    "\n",
    "all_misconceptions = [\n",
    "    (idx + 1, mis, t) for idx, (mis, t) in itertools.chain(enumerate(zip(c2b_misconceptions, itertools.repeat('convertToBinary'))), enumerate(zip(main_misconceptions, itertools.repeat('main'))))\n",
    "]\n",
    "tags = pd.DataFrame(all_misconceptions, columns=['Number', 'Misconception', 'Method'])\n",
    "print(tags.set_index(['Method', 'Number']).style.to_latex())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests_c2b = [\n",
    "    ('Does not use the Math.pow() function', ['!1', '!3']),\n",
    "    ('Can handle INT MAX and get the correct answer', ['!1', '!2', '!3', '!4']),\n",
    "    ('Did not test negative numbers', ['5']),\n",
    "    ('Does not double 0 into 00', ['!4']),\n",
    "    ('Can handle INT MAX', ['!1', '!2', '!3'])\n",
    "]\n",
    "\n",
    "tests_main = [\n",
    "    ('Reads positive numbers and continues reading', ['!1', '!4']),\n",
    "    ('Reads positive and negative numbers and continues reading until -1', ['!1', '!4', '!5']),\n",
    "    ('Does not use System.exit to leave the program', ['!2']),\n",
    "    ('Scanner used correctly', ['!3'])\n",
    "]\n",
    "\n",
    "all_tests = [\n",
    "    (idx + 1, t, method, ', '.join(implications)) for idx, ((t, implications), method) in itertools.chain(enumerate(zip(tests_c2b, itertools.repeat('convertToBinary'))), enumerate(zip(tests_main, itertools.repeat('main'))))\n",
    "]\n",
    "tags = pd.DataFrame(all_tests, columns=['Number', 'Test', 'Method', 'Implied Misconceptions'])\n",
    "print(tags.set_index(['Method', 'Number']).style.to_latex())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    % of submissions that passed this test\nDoes not calculate the number of digits                                           0.907821\nReads positive numbers and continues reading                                      0.874302\nCan handle INT MAX and get the correct answer                                     0.868715\nDid not test negative numbers                                                     0.734637\nOutput matches correct implementation                                             0.874302\nCompiles                                                                          0.974860\nReads positive and negative numbers and continu...                                0.712291\nDoes not double 0 into 00                                                         0.882682\nDoes not use System.exit to leave the program                                     0.703911\nCan handle INT MAX                                                                0.891061\nScanner used correctly                                                            0.921788",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>% of submissions that passed this test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Does not calculate the number of digits</th>\n      <td>0.907821</td>\n    </tr>\n    <tr>\n      <th>Reads positive numbers and continues reading</th>\n      <td>0.874302</td>\n    </tr>\n    <tr>\n      <th>Can handle INT MAX and get the correct answer</th>\n      <td>0.868715</td>\n    </tr>\n    <tr>\n      <th>Did not test negative numbers</th>\n      <td>0.734637</td>\n    </tr>\n    <tr>\n      <th>Output matches correct implementation</th>\n      <td>0.874302</td>\n    </tr>\n    <tr>\n      <th>Compiles</th>\n      <td>0.974860</td>\n    </tr>\n    <tr>\n      <th>Reads positive and negative numbers and continues reading until -1</th>\n      <td>0.712291</td>\n    </tr>\n    <tr>\n      <th>Does not double 0 into 00</th>\n      <td>0.882682</td>\n    </tr>\n    <tr>\n      <th>Does not use System.exit to leave the program</th>\n      <td>0.703911</td>\n    </tr>\n    <tr>\n      <th>Can handle INT MAX</th>\n      <td>0.891061</td>\n    </tr>\n    <tr>\n      <th>Scanner used correctly</th>\n      <td>0.921788</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df = pd.DataFrame(tags)\n",
    "\n",
    "pd.DataFrame(pd.DataFrame(tag_df.transpose()['target'].to_list(), columns=tags_titles).sum(0) / len(tags), columns=['% of submissions that passed this test'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
