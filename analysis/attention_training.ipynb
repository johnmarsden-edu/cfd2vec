{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[('deepwalk', 'FC'),\n ('deepwalk', 'VC'),\n ('deepwalk', 'LC'),\n ('deepwalk', 'NC'),\n ('node2vec', 'FC'),\n ('node2vec', 'VC'),\n ('node2vec', 'LC'),\n ('node2vec', 'NC')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "embedding_methods = [\n",
    "    embedding_method\n",
    "    for embedding_method\n",
    "    in itertools.product(\n",
    "        ['deepwalk', 'node2vec'],\n",
    "        ['FC', 'VC', 'LC', 'NC']\n",
    "    )\n",
    "]\n",
    "\n",
    "embedding_methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 19:56:35.758159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-11 19:56:36.009199: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-11 19:56:36.723200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-11 19:56:36.723275: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-11 19:56:36.723282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-09-11 19:57:18.065918: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-09-11 19:57:18.065970: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pop-os\n",
      "2022-09-11 19:57:18.065979: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pop-os\n",
      "2022-09-11 19:57:18.066070: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.48.7\n",
      "2022-09-11 19:57:18.066094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.48.7\n",
      "2022-09-11 19:57:18.066102: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.48.7\n",
      "2022-09-11 19:57:18.066436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Iterable, Generator\n",
    "from pathlib import Path\n",
    "from psycopg.rows import namedtuple_row\n",
    "from psycopg import Connection\n",
    "import psycopg as pg\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "from logging import debug\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel('INFO')\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "# Add node training dataset\n",
    "\n",
    "save_dir = Path.cwd() / 'save'\n",
    "model_dir = Path.cwd() / '..' / 'data' /'models'\n",
    "score_dir = Path.cwd() / '..' / 'data' / 'scores'\n",
    "\n",
    "score_fname = score_dir / 'codeworkout_program_scores.csv'\n",
    "score_mapper = {}\n",
    "\n",
    "strats = ['VC', 'LC', 'FC', 'NC']\n",
    "\n",
    "\n",
    "def get_scores(programs: Iterable[str], ) -> list[int]:\n",
    "    if len(score_mapper) == 0:\n",
    "        with open(score_fname, 'r') as score_file:\n",
    "            score_reader = csv.DictReader(score_file)\n",
    "            for score in score_reader:\n",
    "                score_mapper[score['program_id']] = float(score['score'])\n",
    "    return [score_mapper[program_id] for program_id in programs]\n",
    "\n",
    "\n",
    "def content_string(node_type: str, label: Optional[str], contents: Optional[str]) -> str:\n",
    "    if node_type == 'Source':\n",
    "        return f'Source {contents}'\n",
    "    if node_type == 'Sink':\n",
    "        return f'Sink {contents}'\n",
    "    out_label = f'{label}: ' if label else ''\n",
    "    out_contents = contents if contents else ''\n",
    "    return f'{out_label}{out_contents}'\n",
    "\n",
    "\n",
    "def get_graph_batch_cursor(conn: Connection, strat: str, limit: int, last_row: Optional[str]):\n",
    "    return conn.execute('''\n",
    "select\n",
    "    p.program_id as program,\n",
    "    g.graph_id as graph,\n",
    "    n.contents as content,\n",
    "    n.label as label,\n",
    "    n.node_type as ntype\n",
    "from\n",
    "    programs as p\n",
    "        join\n",
    "    graphs as g on p.id = g.program_id\n",
    "        join\n",
    "    nodes as n on g.id = n.graph_id\n",
    "where\n",
    "    p.program_id in (\n",
    "        select\n",
    "            program_id\n",
    "        from\n",
    "            programs\n",
    "        where\n",
    "                    program_id like %(strat)s\n",
    "          and\n",
    "                    ( cast(%(last_row)s as varchar) IS NULL or program_id > %(last_row)s)\n",
    "        order by\n",
    "            program_id\n",
    "        limit\n",
    "            %(limit)s\n",
    "    )\n",
    "''', {'strat': f'{strat}-%', 'limit': limit, 'last_row': last_row}, prepare=True)\n",
    "\n",
    "\n",
    "db_dataset_shapes = {}\n",
    "\n",
    "\n",
    "def get_db_dataset_shapes(conn: Connection):\n",
    "    if len(db_dataset_shapes) != 0:\n",
    "        return db_dataset_shapes\n",
    "    cursor = conn.execute(\"\"\"\n",
    "-- dimension 0 - programs\n",
    "with dim0 as (\n",
    "    select\n",
    "        count(*) as dim, substring(p.program_id for 2) as strat\n",
    "    from\n",
    "        programs as p\n",
    "    group by\n",
    "        substring(p.program_id for 2)\n",
    "),\n",
    "\n",
    "-- dimension 1 - max number of graphs\n",
    "dim1 as (\n",
    "    select\n",
    "        max(graph_counts.num_graphs) as dim, graph_counts.strat\n",
    "    from\n",
    "        (select count(*) as num_graphs, substring(p.program_id for 2) as strat\n",
    "        from\n",
    "            programs as p\n",
    "                join graphs as g on p.id = g.program_id\n",
    "        group by p.program_id) as graph_counts\n",
    "    group by\n",
    "        graph_counts.strat\n",
    "),\n",
    "\n",
    "-- dimension 2 - max number of nodes (dimension 3 is the node embeddings which are size 50)\n",
    "dim2 as (\n",
    "    select\n",
    "        max(node_counts.num_nodes) as dim, node_counts.strat\n",
    "    from\n",
    "        (\n",
    "            select\n",
    "                count(*) as num_nodes, substring(p.program_id for 2) as strat\n",
    "            from\n",
    "                programs as p\n",
    "                    join graphs g on p.id = g.program_id\n",
    "                    join nodes as n on g.id = n.graph_id\n",
    "            group by\n",
    "                p.program_id\n",
    "        ) as node_counts\n",
    "    group by\n",
    "        node_counts.strat\n",
    ")\n",
    "\n",
    "select\n",
    "    dim0.strat as strat, dim0.dim as dim0, dim1.dim as dim1, dim2.dim as dim2, 50 as dim3\n",
    "from\n",
    "    dim0\n",
    "        join dim1 on dim0.strat = dim1.strat\n",
    "        join dim2 on dim0.strat = dim1.strat\n",
    "    \"\"\")\n",
    "    cursor.row_factory = namedtuple_row\n",
    "\n",
    "    shapes = cursor.fetchall()\n",
    "    for shape in shapes:\n",
    "        db_dataset_shapes[shape.strat] = (shape.dim0, shape.dim1, shape.dim2, shape.dim3)\n",
    "    return db_dataset_shapes\n",
    "\n",
    "\n",
    "def get_graph_batches(conn: Connection, w2v: Word2Vec, shapes: dict[str, tuple[int, int, int, int]], strat: str, batch_size: int) -> Generator[tf.Tensor, None, None]:\n",
    "    processed = 0\n",
    "    last_row = None\n",
    "    while processed < shapes[strat][0]:\n",
    "        processed += batch_size\n",
    "        programs = {}\n",
    "        cursor = get_graph_batch_cursor(conn, strat, batch_size, last_row)\n",
    "        cursor.row_factory = namedtuple_row\n",
    "        for row in cursor:\n",
    "            if not row.program in programs:\n",
    "                programs[row.program] = {}\n",
    "            if not row.graph in programs[row.program]:\n",
    "                programs[row.program][row.graph] = []\n",
    "            content_str = content_string(row.ntype, row.label, row.content)\n",
    "            if content_str in w2v.wv:\n",
    "                node_embedding = w2v.wv[content_str]\n",
    "            else:\n",
    "                node_embedding = np.zeros(50)\n",
    "            programs[row.program][row.graph].append(node_embedding)\n",
    "            last_row = row.program\n",
    "        yield programs\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "feature_description = {\n",
    "    'program': tf.io.FixedLenFeature((), tf.string),\n",
    "    'score': tf.io.FixedLenFeature((), tf.float32, default_value=0.0)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_graph_tfrecord(example_proto):\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    debug(\"Example: \", example)\n",
    "    wrapper = tf.io.parse_tensor(example['program'], out_type=tf.string)\n",
    "    debug(\"Wrapper: \", wrapper)\n",
    "    indices = tf.io.parse_tensor(wrapper[0], out_type=tf.int64)\n",
    "    debug(\"Indices: \", indices)\n",
    "    values = tf.io.parse_tensor(wrapper[1], out_type=tf.float32)\n",
    "    debug(values)\n",
    "    shape = tf.io.parse_tensor(wrapper[2], out_type=tf.int64)\n",
    "    debug(shape)\n",
    "    sparse = tf.sparse.SparseTensor(indices, values, shape)\n",
    "    debug(sparse)\n",
    "    dense = tf.sparse.to_dense(sparse)\n",
    "    debug(dense)\n",
    "    return dense, example['score']\n",
    "\n",
    "\n",
    "def get_graph_dataset(conn: Connection, ds_loc: Path, shapes: dict[str, tuple[int, int, int, int]], strat: str, embed_method: str, batch_size: int):\n",
    "    if not ds_loc.exists():\n",
    "        w2v = Word2Vec.load(str(model_dir / f'{strat}-{embed_method}.model'))\n",
    "        strat_strip_length = len(f'{strat}-')\n",
    "        with tf.io.TFRecordWriter(str(ds_loc)) as data_writer:\n",
    "            for programs in get_graph_batches(conn, w2v, shapes, strat, batch_size):\n",
    "                scores = get_scores(map(lambda key: key[strat_strip_length:], programs.keys()))\n",
    "                for (graphs, score) in zip(programs.values(), scores):\n",
    "                    ragged_tensor = tf.ragged.constant([nodes for nodes in graphs.values()])\n",
    "                    debug(\"Ragged tensor: \", ragged_tensor)\n",
    "                    sparse_tensor = ragged_tensor.to_sparse()\n",
    "                    debug(\"Sparse tensor: \", sparse_tensor)\n",
    "                    sparse_tensor = tf.sparse.SparseTensor(sparse_tensor.indices, sparse_tensor.values, shapes[strat][1:])\n",
    "                    debug(\"Reshaped Sparse Tensor\", sparse_tensor)\n",
    "                    sparse_tensor_tensor = tf.io.serialize_sparse(sparse_tensor)\n",
    "                    debug(\"Sparse tensor tensor: \", sparse_tensor_tensor)\n",
    "                    serialized_program_tensor = tf.io.serialize_tensor(sparse_tensor_tensor)\n",
    "                    debug(\"Serialized program tensor: \", serialized_program_tensor)\n",
    "                    example_proto = tf.train.Example(\n",
    "                        features=tf.train.Features(feature={\n",
    "                            'program': _bytes_feature(serialized_program_tensor),\n",
    "                            'score': _float_feature(score)\n",
    "                        })\n",
    "                    )\n",
    "\n",
    "                    debug(example_proto)\n",
    "                    data_writer.write(example_proto.SerializeToString())\n",
    "\n",
    "    return tf.data.TFRecordDataset([str(ds_loc)]).prefetch(buffer_size=tf.data.AUTOTUNE).map(_parse_graph_tfrecord).batch(batch_size)\n",
    "\n",
    "\n",
    "    #\n",
    "    # ds = tf.data.Dataset.from_tensor_slices((programs, scores), name=f'{strat}_{embed_method}')\n",
    "    # ds.save(str(dataset_loc))\n",
    "    # with open(f'{dataset_loc}.shape', 'w') as dataset_shape_file:\n",
    "    #     dataset_shape_file.write(f'{programs.shape}')\n",
    "    # return ds\n",
    "\n",
    "# TODO: Calculate the size of the tensor, batch the creation of the tensors into multiple steps.\n",
    "# Consider using a generator for the dataset instead of loading it all into memory before saving it.\n",
    "# If we use a generator, it should be able to automatically delete anything that takes up too much\n",
    "# memory while it saves to disk.\n",
    "def get_graph_datasets(batch_size: int) -> Generator[tf.data.Dataset, None, None]:\n",
    "    with pg.connect() as conn:\n",
    "        for embed_method, strat in embedding_methods:\n",
    "            db_shapes = get_db_dataset_shapes(conn)\n",
    "            yield get_graph_dataset(conn, save_dir / f'{strat}_{embed_method}.tfrecord', get_db_dataset_shapes(conn), strat, embed_method, batch_size), db_shapes[strat]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "datasets_info = [(dataset, shape) for dataset, shape in get_graph_datasets(BATCH_SIZE)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test = ''\n",
    "# for element in datasets_info[0].take(1):\n",
    "#     test = element\n",
    "# test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graph Embedding Model\n",
    "# 1 ex. input:\n",
    "# [\n",
    "#   [1 .. 1],\n",
    "#   ...,\n",
    "#   [0 .. 2]\n",
    "# ]\n",
    "class AttentionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, ishape):\n",
    "        super(AttentionEmbedding, self).__init__()\n",
    "        self.ishape = ishape\n",
    "        debug('AE ishape: ', ishape)\n",
    "        self.mha = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2, name='embedding_attention_layer')\n",
    "        self.summer = keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        inputs = tf.expand_dims(inputs, axis=0)\n",
    "        debug(\"AE - call - self.ishape: \", self.ishape)\n",
    "        debug(\"AE - call - inputs.shape: \", inputs.shape)\n",
    "        tensors = self.mha(inputs, inputs, training=kwargs['training'])\n",
    "        return self.summer(tf.unstack(tf.squeeze(tensors, axis=0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ProgramsEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, ishape, nodes_shape, graphs_shape):\n",
    "        super(ProgramsEmbedding, self).__init__()\n",
    "        self.ishape = ishape\n",
    "        self.embed_graph = AttentionEmbedding(nodes_shape)\n",
    "        self.embed_program = AttentionEmbedding(graphs_shape)\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        programs = tf.TensorArray(size=self.ishape[0], dtype=tf.float32)\n",
    "        debug('PE - call - input_shape: ', self.ishape)\n",
    "        for i in tf.range(self.ishape[0]):\n",
    "            graphs = tf.TensorArray(dtype=tf.float32, size=self.ishape[1])\n",
    "            for j in tf.range(self.ishape[1]):\n",
    "                graph = self.embed_graph(inputs[i][j], training=kwargs['training'])\n",
    "                debug('PE - call - graph.shape: ', graph.shape)\n",
    "                graphs.write(j, graph)\n",
    "            graphs = graphs.stack()\n",
    "            debug('PE - call - graphs.shape: ' , graphs.shape)\n",
    "            programs.write(i, self.embed_program(graphs))\n",
    "        return programs.stack()\n",
    "\n",
    "\n",
    "# Program Embedding Model\n",
    "# 1 ex. input:\n",
    "# [\n",
    "#   [\n",
    "#     [0 .. 1],\n",
    "#     [1 .. 4]\n",
    "#   ],\n",
    "#   [\n",
    "#     [1 .. 2],\n",
    "#     [0 .. 0]\n",
    "#   ]\n",
    "# ]\n",
    "for dataset, dataset_shape in datasets_info:\n",
    "    logging.info('Creating program input layer')\n",
    "    program_input = keras.Input(shape=dataset_shape[1:], ragged=True, name='programs')\n",
    "\n",
    "    debug(program_input.shape)\n",
    "    logging.info('Creating programs embedding layer')\n",
    "    program_embeddings = ProgramsEmbedding((BATCH_SIZE, *dataset_shape), dataset_shape[2:], (dataset_shape[1], dataset_shape[3]))\n",
    "\n",
    "    logging.info('Connecting program input layer to program embeddings layer')\n",
    "    embedded = program_embeddings(program_input)\n",
    "\n",
    "    logging.info('Creating an output layer and connecting it to the program embedding layer')\n",
    "    program_output = keras.layers.Dense(units=1, name='scores')(embedded)\n",
    "\n",
    "    logging.info('Creating the model that we will be training')\n",
    "    program_model = keras.Model(inputs=program_input, outputs=program_output, name='program_embedding_model')\n",
    "\n",
    "    logging.info('Printing program model summary')\n",
    "    program_model.summary()\n",
    "\n",
    "    logging.info('Compile the model')\n",
    "    program_model.compile(optimizer='adam',\n",
    "                          loss='mse')\n",
    "\n",
    "    logging.info('Fit the model to the dataset')\n",
    "    program_model.fit(dataset, epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}